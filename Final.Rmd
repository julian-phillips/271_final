---
title: "271 Final"
author: "Glenn (Ted) Dunmire"
date: "December 7, 2015"
output: html_document
---

```{r}
setwd("~/Documents/271 Final")
library(ggplot2)
library(reshape2)
library(grid)
library(astsa)
library(forecast)
library(quantmod)
library(fGarch)
library(tseries)
```

##Question 1

_Analyze each of these variables (as well as a combination of them) very carefully and use them (or a subset of them) to build a model and test hypotheses to address the questions. Also address potential (statistical) issues that may be casued by omitted variables._
_The philanthropist group hires a think tank to examine the relationship between the house values and neighborhood characteristics. For instance, they are interested in the extent to which houses in neighbhorhood with desirable features command higher values. They are specifically interested in environmental features, such as proximity to water body (i.e. lake, river, or ocean) or air quality._

```{r}
data <- read.csv("houseValueData.csv")
data$withWater <- as.factor(data$withWater) #changed to factor based on documentation
```

Let us first begin with some basic examination of the data to see what kinds of variables we have and what their distributions look like. We have turned the withWater variable into a factor based on the documentation, because it is a categorical variable rather than an int. 

```{r}
str(data)
sum(is.na(data))
summary(data)
```

Notice we have 400 observations of 11 variables, with no missing values. 

```{r}
ggplot(melt(data[,-3]), aes(value)) + geom_histogram(color = "black", fill = "white") + facet_wrap(~variable, scales = "free") + labs(title = "Histogram of Variables")

table(data$withWater)
```

```{r}
#Scatterplots of variables against log of home Value
data2 <- data
data2$logHomeValue <- log(data2$homeValue) #Log transform homeValue

plot_scatterplot = function(pl) {
  return(pl + geom_point(size=1.5) + 
           geom_smooth(method="lm", alpha=0.2) +
           theme(legend.key.size=unit(.3, "cm")))
}

crimePC.value.plot <- ggplot(data2, aes(crimeRate_pc, logHomeValue))
crimePC.value.plot = plot_scatterplot(crimePC.value.plot)

nonRetail.value.plot <- ggplot(data2, aes(nonRetailBusiness, logHomeValue))
nonRetail.value.plot = plot_scatterplot(nonRetail.value.plot)

ageHouse.value.plot <- ggplot(data2, aes(ageHouse, logHomeValue))
ageHouse.value.plot = plot_scatterplot(ageHouse.value.plot)

distanceCity.value.plot <- ggplot(data2, aes(distanceToCity, logHomeValue))
distanceCity.value.plot = plot_scatterplot(distanceCity.value.plot)

distanceHighway.value.plot <- ggplot(data2, aes(distanceToHighway, logHomeValue))
distanceHighway.value.plot = plot_scatterplot(distanceHighway.value.plot)

pupilTeacher.value.plot <- ggplot(data2, aes(pupilTeacherRatio, logHomeValue))
pupilTeacher.value.plot = plot_scatterplot(pupilTeacher.value.plot)

lowIncome.value.plot <- ggplot(data2, aes(pctLowIncome, logHomeValue))
lowIncome.value.plot = plot_scatterplot(lowIncome.value.plot)

pollution.value.plot <- ggplot(data2, aes(pollutionIndex, logHomeValue))
pollution.value.plot = plot_scatterplot(pollution.value.plot)

nBedrooms.value.plot <- ggplot(data2, aes(nBedRooms, logHomeValue))
nBedrooms.value.plot = plot_scatterplot(nBedrooms.value.plot)
```

#reconsider to use homeValue?

#Possible categorical factors in crimeRate_pc (x < 1), ageHouse (x > 95), distanceToCity, distanceToHighway


##Question 2

_Build a time-series model for the series in series02.txt and use it to perform a 24-step ahead forecast._

```{r}
series <- read.table("series02.txt")
series <- ts(series$V1)

#Visualize the data
par(mfrow = c(2,2))
plot.ts(series, col = "navy", xlab = "Time Period", ylab = "Values", main = "Time Series for Series 02")
hist(series, main = "Histogram of Values of Series 02")
acf(series, main = "ACF of Series 02")
pacf(series, main = "PACF of Series 02")
```

Notice the general structure of the series. There seems to be a long run average, where the values are fluctuating around a central axis but with with a major series of spikes in the beginning signaling serious volatility. There does not seem to be seasonality or a trend. The ACF interestingly shows a sharp drop after the 0 lag, but slightly statistically significant lags throughout the series. The PACF also shows slight significance at several lags after the most significant at what looks like the 3rd lag. 

We suspect there is non-constant variance present in this series, so we will plot a correlogram of the squared values of a mean adjusted version of this series (adjusted so the mean is zero). 

```{r}
par(mfrow = c(1,1))
acf((series - mean(series))^2, main = "ACF of Squared Terms")
```

The square values that are plotted are equivalent to the variance. What the statistically significant values indicate is that there is serial correlation, meaning conditional heteroskedasticity. In plain English, this means that the variance is not constant throughout the series, rather the variance depends on what window of time we are looking at. This violates a core assumption of stationarity, meaning we will have to use a non-stationary model to fit this data. 

```{r}
garch1 <- garch(series, trace = F)
resids <- garch1$residuals[-1] #first value is NA

par(mfrow = c(2,1))
acf(resids, main = "ACF of GARCH(1,1) Residuals")
acf(resids^2, main = "ACF of GARCH(1,1) Residuals^2")
Box.test(resids, type = "Ljung-Box")
garch1
t(confint(garch1))
```

Notice that with the ACF of both the series residuals and sqaured residuals there is no autocorrelation. This suggests the residuals are behaving like white noise and thus the model is a good fit. Examining the model itself 0 is not contained in the 95% confidence intervals for the coefficients, meaning that the coefficients are satatistically significant at the 95% level. 

#Question: Do we think that's good enough? Look into fitting GARCH better
#Forecasting

##Question 3

_Build a time-series model for the series in series03.csv and use it to perform a 24-step ahead forecast_

```{r}
#load data and do preliminary visualization
series <- read.csv("series03.csv")
series <- ts(series$X9.88)

par(mfrow = c(2,2))
plot.ts(series, xlab = "Time Period", ylab = "Value", main = "Time Series Plot of Series 03", col = "navy")
hist(series, main = "Histogram of Series 03")
acf(series, main = "ACF of Series 03")
pacf(series, main = "PACF of Series 03")
```

Notice from the time series plot that there is significant trend going on, long term upwards. The ACF shows significance through all past lags while the PACF is only siginficant for the first lag. There does not seem to be any seasonality. This looks like the realization of a random walk with drift process. 


```{r}
series2 <- diff(log(series)) #I think this should be difference = 2
plot.ts(series2)
hist(series2)
acf(series2)
pacf(series2)

#difference of log using ARIMA and then GARCh for residuals but not sure how to forecast
```

First we will take the difference of the log of the series. The point of doing a difference is to remove the trend of the series and make it stationary, allowing us to fit a model to it. The first difference of a random walk process is white noise and by definition is stationary, so that seems like a good starting point to investigate. The time series plot of the differenced series looks much more like white noise, although it seems very volatile. The ACF shows no statistically significant autocorrelations, although the partial autocorrelations do show significance, particularly at the 15th and 20th lags. 

```{r}
#Fit ARIMA model 
#Function to fit arima model based on parameters, credit Jeffrey Yau
get.best.arima <- function(x.ts, maxord = c(1,1,1))
{
  best.aic <- 1e8
  n <- length(x.ts)
  for (p in 0:maxord[1]) for (d in 0:maxord[2]) for (q in 0:maxord[3])
  {
    fit <- arima(x.ts, order = c(p, d, q))
    fit.aic <- -2 * fit$loglik + (log(n) + 1) * length(fit$coef)
    if (fit.aic < best.aic)
    {
      best.aic <- fit.aic
      best.fit <- fit
      best.model <- c(p, d, q)
    }
  }
  list(best.aic, best.fit, best.model)
}

arima.model <- get.best.arima(series, maxord = rep(2, 3))
arima.model2 <- auto.arima(log(series), allowdrift = FALSE)
arima.model
arima.model2
```

Both the method provided by Jeff and the auto arima function suggest using a 1st difference ARIMA model with no AR or MA terms to model the log of the series. 

#No parameters?

```{r}
model <- arima(log(series), order = c(0, 1, 0))

#diagnostic plots of residuals
resids <- model$residuals
plot.ts(resids, main = "Residuals of ARIMA Model")
hist(resids, main = "Histogram of Residuals")
acf(resids, main = "ACF of Residuals")
pacf(resids, main = "PACF of Residuals")
```



```{r}
get.best.arima.seas <- function(x.ts, maxord = c(1,1,1,1,1,1)) {
  best.aic <- 1e8
  n <- length(x.ts)
  for (p in 0:maxord[1]) for(d in 0:maxord[2]) for(q in 0:maxord[3])
    for (P in 0:maxord[4]) for(D in 0:maxord[5]) for(Q in maxord[6])
    {
      fit <- arima(x.ts, order = c(p, d, q), seas = list(order = c(P,D,Q), frequency(x.ts)), method = "CSS")
      fit.aic <- -2 * fit$loglik + (log(n) + 1) * length(fit$coef)
      if (fit.aic < best.aic)
      {
        best.aic <- fit.aic
        best.fit <- fit
        best.model <- c(p, d, q, P, D, Q)
      }
    }
  list(best.aic, best.fit, best.model)
}

```

Step 1:  Do a time series plot of the data.  Examine it for features such as trend and seasonality.  You’ll know that you’ve gathered seasonal data (months, quarters, etc,) so look at the pattern across those time units (months, etc.) to see if there is indeed a seasonal pattern.

Step 2:  Do any necessary differencing. The general guidelines are:

If there is seasonality and no trend take a difference of lag S. For instance, take a 12th difference for monthly data with seasonality.
If there is linear trend and no obvious seasonality, take a first difference.  If there is a curved trend, consider a transformation of the data before differencing.
If there is both trend and seasonality, apply both a non-seasonal and seasonal difference to the data, as two successive operations (in either order).  For instance, if the series is called x, a successive first and 12th difference in R would be:
diff1 = diff(x, 1)
diff1and12=diff(diff1, 12)

If there is neither obvious trend nor seasonality, don’t take any differences.
Step 3:  Examine the ACF and PACF of the differenced data (if differencing is necessary).

We’re using this information to determine possible models.  This can be tricky going involving some (educated) guessing.  Some basic guidance:

Non-seasonal terms:  Examine the early lags (1, 2, 3, …) to judge non-seasonal terms.  Spikes in the ACF (at low lags) indicate non-seasonal MA terms.  Spikes in the PACF (at low lags) indicated possible non-seasonal AR terms.

Seasonal terms:  Examine the patterns across lags that are multiples of S. For example, for monthly data, look at lags 12, 24, 36, and so on (probably won’t need to look at much more than the first two or three seasonal multiples).  Judge the ACF and PACF at the seasonal lags in the same way you do for the earlier lags.

Step 4:  Estimate the model(s) that might be reasonable on the basis of Step 3.  Don’t forget to include any differencing that you did before looking at the ACF and PACF.  In the software, specify the original series as the data and then indicate the desired differencing when specifying parameters in the arima command that you’re using.

Step 5:  Examine the residuals (with ACF, Box-Pierce, and any other means) to see if the model seems good.  Compare AIC or BIC values if you tried several models.

If things don’t look good here, it’s back to Step 3 (or maybe even Step 2).

#Looks like the previous (week 11) homework, ARIMA of log?


