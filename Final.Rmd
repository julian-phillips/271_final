---
title: "271 Final"
author: "Glenn (Ted) Dunmire"
date: "December 7, 2015"
output: html_document
---

```{r}
setwd("~/Documents/271 Final")
library(ggplot2)
library(reshape2)
library(grid)
library(astsa)
library(forecast)
library(quantmod)
library(fGarch)
library(tseries)
```

##Question 1

_Analyze each of these variables (as well as a combination of them) very carefully and use them (or a subset of them) to build a model and test hypotheses to address the questions. Also address potential (statistical) issues that may be casued by omitted variables._
_The philanthropist group hires a think tank to examine the relationship between the house values and neighborhood characteristics. For instance, they are interested in the extent to which houses in neighbhorhood with desirable features command higher values. They are specifically interested in environmental features, such as proximity to water body (i.e. lake, river, or ocean) or air quality._

```{r}
data <- read.csv("houseValueData.csv")
data$withWater <- as.factor(data$withWater) #changed to factor based on documentation
```

Let us first begin with some basic examination of the data to see what kinds of variables we have and what their distributions look like. We have turned the withWater variable into a factor based on the documentation, because it is a categorical variable rather than an int. 

```{r}
str(data)
sum(is.na(data))
summary(data)
```

Notice we have 400 observations of 11 variables, with no missing values. 

```{r}
ggplot(melt(data[,-3]), aes(value)) + geom_histogram(color = "black", fill = "white") + facet_wrap(~variable, scales = "free") + labs(title = "Histogram of Variables")

table(data$withWater)
```

```{r}
#Scatterplots of variables against log of home Value
data2 <- data
data2$logHomeValue <- log(data2$homeValue) #Log transform homeValue

plot_scatterplot = function(pl) {
  return(pl + geom_point(size=1.5) + 
           geom_smooth(method="lm", alpha=0.2) +
           theme(legend.key.size=unit(.3, "cm")))
}

crimePC.value.plot <- ggplot(data2, aes(crimeRate_pc, logHomeValue))
crimePC.value.plot = plot_scatterplot(crimePC.value.plot)

nonRetail.value.plot <- ggplot(data2, aes(nonRetailBusiness, logHomeValue))
nonRetail.value.plot = plot_scatterplot(nonRetail.value.plot)

ageHouse.value.plot <- ggplot(data2, aes(ageHouse, logHomeValue))
ageHouse.value.plot = plot_scatterplot(ageHouse.value.plot)

distanceCity.value.plot <- ggplot(data2, aes(distanceToCity, logHomeValue))
distanceCity.value.plot = plot_scatterplot(distanceCity.value.plot)

distanceHighway.value.plot <- ggplot(data2, aes(distanceToHighway, logHomeValue))
distanceHighway.value.plot = plot_scatterplot(distanceHighway.value.plot)

pupilTeacher.value.plot <- ggplot(data2, aes(pupilTeacherRatio, logHomeValue))
pupilTeacher.value.plot = plot_scatterplot(pupilTeacher.value.plot)

lowIncome.value.plot <- ggplot(data2, aes(pctLowIncome, logHomeValue))
lowIncome.value.plot = plot_scatterplot(lowIncome.value.plot)

pollution.value.plot <- ggplot(data2, aes(pollutionIndex, logHomeValue))
pollution.value.plot = plot_scatterplot(pollution.value.plot)

nBedrooms.value.plot <- ggplot(data2, aes(nBedRooms, logHomeValue))
nBedrooms.value.plot = plot_scatterplot(nBedrooms.value.plot)
```

#reconsider to use homeValue?

#Possible categorical factors in crimeRate_pc (x < 1), ageHouse (x > 95), distanceToCity, distanceToHighway


##Question 2

_Build a time-series model for the series in series02.txt and use it to perform a 24-step ahead forecast._

```{r}
series <- read.table("series02.txt")
series <- ts(series$V1)

#Visualize the data
par(mfrow = c(2,2))
plot.ts(series, col = "navy", xlab = "Time Period", ylab = "Values", main = "Time Series for Series 02")
hist(series, main = "Histogram of Values of Series 02")
acf(series, main = "ACF of Series 02")
pacf(series, main = "PACF of Series 02")
```

Notice the general structure of the series. There seems to be a long run average, where the values are fluctuating around a central axis but with with a major series of spikes in the beginning signaling serious volatility. There does not seem to be seasonality or a trend. The ACF interestingly shows a sharp drop after the 0 lag, but slightly statistically significant lags throughout the series. The PACF also shows slight significance at several lags after the most significant at what looks like the 3rd lag. 

We suspect there is non-constant variance present in this series, so we will plot a correlogram of the squared values of a mean adjusted version of this series (adjusted so the mean is zero). 

```{r}
par(mfrow = c(1,1))
acf((series - mean(series))^2, main = "ACF of Squared Terms")
```

The square values that are plotted are equivalent to the variance. What the statistically significant values indicate is that there is serial correlation, meaning conditional heteroskedasticity. In plain English, this means that the variance is not constant throughout the series, rather the variance depends on what window of time we are looking at. This violates a core assumption of stationarity, meaning we will have to use a non-stationary model to fit this data. 

```{r}
garch1 <- garch(series, trace = F)
resids <- garch1$residuals[-1] #first value is NA

par(mfrow = c(2,1))
acf(resids, main = "ACF of GARCH(1,1) Residuals")
acf(resids^2, main = "ACF of GARCH(1,1) Residuals^2")
Box.test(resids, type = "Ljung-Box")
garch1
t(confint(garch1))
```

Notice that with the ACF of both the series residuals and sqaured residuals there is no autocorrelation. This suggests the residuals are behaving like white noise and thus the model is a good fit. Examining the model itself 0 is not contained in the 95% confidence intervals for the coefficients, meaning that the coefficients are satatistically significant at the 95% level. 

#Question: Do we think that's good enough? Look into fitting GARCH better
#Forecasting

##Question 3

_Build a time-series model for the series in series03.csv and use it to perform a 24-step ahead forecast_

```{r}
#load data and do preliminary visualization
series <- read.csv("series03.csv")
series <- ts(series$X9.88)

par(mfrow = c(2,2))
plot.ts(series, xlab = "Time Period", ylab = "Value", main = "Time Series Plot of Series 03", col = "navy")
hist(series, main = "Histogram of Series 03")
acf(series, main = "ACF of Series 03")
pacf(series, main = "PACF of Series 03")
```

Notice from the time series plot that there is significant trend going on, long term upwards. The ACF shows significance through all past lags while the PACF is only siginficant for the first lag. There does not seem to be any seasonality. This looks like the realization of a random walk with drift process. 


```{r}
#plot different time series to suggest differencing
par(mfrow = c(3, 2))
plot.ts(series, main = "Original Time Series")
plot.ts(log(series), main = "Log of Time Series")
plot.ts(diff(series), main = "First Difference of Time Series")
plot.ts(diff(series, d = 2), main = "Second Difference of Time Series")
plot.ts(diff(log(series)), main = "First Difference of Log Time Series")
plot.ts(diff(log(series), d = 2), main = "Second Difference of Log Time Series")
```

It is clear from the original time series plot that the series is not stationary. Before proceeding to build a model we must render the series as stationary. 

```{r}
par(mfrow = c(2,2))
acf(diff(series), main = "ACF of First order Difference")
pacf(diff(series), main = "PACF of First order Difference")
acf(diff(series, d= 2), main = "ACF of Second order Difference")
pacf(diff(series, d = 2), main = "PACF of Second order Difference")

acf(diff(log(series)), main = "ACF of First Order Difference of Log")
pacf(diff(log(series)), main = "PACF of First Order Difference of Log")
acf(diff(log(series), d = 2), main = "ACF of Second Order Difference of Log")
pacf(diff(log(series), d = 2), main = "PACF of Second Order Difference of Log")
```

From examining these plots, it seems as though the second order difference provides the best transformation into white noise. In both cases the ACF shows a sharp cut off (suggesting an MA term) while the PACF gradually declines. The first order difference shows a lot of volatility in the PACF, suggesting correlations that are not easily captured. 
Between the second order difference and the second order difference of the log, the second order difference of the log seems to look more like white noise. There are fewer significant autocorrelations (which might be due to sampling) in the second order difference of the log and it decays more smoothly. Therefore, we will use the second order difference of the log to estimate the model. 

```{r}
get.best.arima <- function(x.ts, maxord = c(1,1,1))
{
  best.aic <- 1e8
  n <- length(x.ts)
  for (p in 0:maxord[1]) for (d in 0:maxord[2]) for (q in 0:maxord[3])
  {
    fit <- arima(x.ts, order = c(p, d, q), method = "ML")
    fit.aic <- -2 * fit$loglik + (log(n) + 1) * length(fit$coef)
    if (fit.aic < best.aic)
    {
      best.aic <- fit.aic
      best.fit <- fit
      best.model <- c(p, d, q)
    }
  }
  list(best.aic, best.fit, best.model)
}

model <- arima(log(series), order = c(0, 2, 1))
model
t(confint(model))
```

0 is not contained in the confidence interval so this coefficient is statistically significant. 


```{r}
#diagnostic plots of residuals
resids <- model$residuals
plot.ts(resids, main = "Residuals of ARIMA Model")
hist(resids, main = "Histogram of Residuals")
acf(resids, main = "ACF of Residuals")
pacf(resids, main = "PACF of Residuals")
```

These residual diagnostics suggest a reasonably good aprpoximation of white noise. The ACF and PACF however do show quite a bit of volatility, so we will examine the squared residuals because we suspect there is non-constant variance. 

```{r}
par(mfrow = c(2,1))
acf(resids, main = "ACF of Residuals")
acf(resids^2, main = "ACF of Squared Residuals")
```

As we had suspected, the squared residuals show statistically significant terms at different intervals. Clearly, this suggests there is non-constant variance. Therefore, we will fit a GARCH model to the residuals. 

```{r}
resid.garch = garch(resids, trace = FALSE)
t(confint(resid.garch))
acf(resid.garch$residuals[-1], main = "ACF of GARCH fitted Residuals")
acf(resid.garch$residuals[-1]^2, main = "ACF of GARCH fitted Residuals^2")
```


The GARCH model shows statistically significant coefficients at the 95% level, because 0 is not contained in the confidence intervals. The ACFs of both the GARCH residuals and residuals squared show no significant values, meaning a good fit to the residuals. 

#Forecast? 
```{r}
get.best.arima.seas <- function(x.ts, maxord = c(1,1,1,1,1,1)) {
  best.aic <- 1e8
  n <- length(x.ts)
  for (p in 0:maxord[1]) for(d in 0:maxord[2]) for(q in 0:maxord[3])
    for (P in 0:maxord[4]) for(D in 0:maxord[5]) for(Q in maxord[6])
    {
      fit <- arima(x.ts, order = c(p, d, q), seas = list(order = c(P,D,Q), frequency(x.ts)), method = "CSS")
      fit.aic <- -2 * fit$loglik + (log(n) + 1) * length(fit$coef)
      if (fit.aic < best.aic)
      {
        best.aic <- fit.aic
        best.fit <- fit
        best.model <- c(p, d, q, P, D, Q)
      }
    }
  list(best.aic, best.fit, best.model)
}

```

Step 1:  Do a time series plot of the data.  Examine it for features such as trend and seasonality.  You’ll know that you’ve gathered seasonal data (months, quarters, etc,) so look at the pattern across those time units (months, etc.) to see if there is indeed a seasonal pattern.

Step 2:  Do any necessary differencing. The general guidelines are:

If there is seasonality and no trend take a difference of lag S. For instance, take a 12th difference for monthly data with seasonality.
If there is linear trend and no obvious seasonality, take a first difference.  If there is a curved trend, consider a transformation of the data before differencing.
If there is both trend and seasonality, apply both a non-seasonal and seasonal difference to the data, as two successive operations (in either order).  For instance, if the series is called x, a successive first and 12th difference in R would be:
diff1 = diff(x, 1)
diff1and12=diff(diff1, 12)

If there is neither obvious trend nor seasonality, don’t take any differences.
Step 3:  Examine the ACF and PACF of the differenced data (if differencing is necessary).

We’re using this information to determine possible models.  This can be tricky going involving some (educated) guessing.  Some basic guidance:

Non-seasonal terms:  Examine the early lags (1, 2, 3, …) to judge non-seasonal terms.  Spikes in the ACF (at low lags) indicate non-seasonal MA terms.  Spikes in the PACF (at low lags) indicated possible non-seasonal AR terms.

Seasonal terms:  Examine the patterns across lags that are multiples of S. For example, for monthly data, look at lags 12, 24, 36, and so on (probably won’t need to look at much more than the first two or three seasonal multiples).  Judge the ACF and PACF at the seasonal lags in the same way you do for the earlier lags.

Step 4:  Estimate the model(s) that might be reasonable on the basis of Step 3.  Don’t forget to include any differencing that you did before looking at the ACF and PACF.  In the software, specify the original series as the data and then indicate the desired differencing when specifying parameters in the arima command that you’re using.

Step 5:  Examine the residuals (with ACF, Box-Pierce, and any other means) to see if the model seems good.  Compare AIC or BIC values if you tried several models.

If things don’t look good here, it’s back to Step 3 (or maybe even Step 2).

#Looks like the previous (week 11) homework, ARIMA of log?


