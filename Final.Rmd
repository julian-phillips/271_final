---
title: "271 Final"
author: "Glenn (Ted) Dunmire, Marlea Gwinn, Julian Phillips"
date: "December 7, 2015"
output: html_document
---

##Question 1

_Analyze each of these variables (as well as a combination of them) very carefully and use them (or a subset of them) to build a model and test hypotheses to address the questions. Also address potential (statistical) issues that may be casued by omitted variables._
_The philanthropist group hires a think tank to examine the relationship between the house values and neighborhood characteristics. For instance, they are interested in the extent to which houses in neighbhorhood with desirable features command higher values. They are specifically interested in environmental features, such as proximity to water body (i.e. lake, river, or ocean) or air quality._


###Preparation for data analysis
```{r}

#Set Directory

#Ted
#setwd("~/Documents/271 Final")

#Marlea
#setwd("C://Users/gwina003/Downloads/Final")

#Julian
#data <- read.csv("//vivica/Documents/MIDS/W271/271-Final/houseValueData.csv")

#Load Relevant Libraries
library(ggplot2)
library(car)
library(reshape2)
library(grid)
library(astsa)
library(forecast)
library(quantmod)
library(fGarch)
library(tseries)
library(gridExtra)
library(scales)
library(plyr)
library(GGally)
library(sandwich)
library(lmtest)
```

###Read data and conduct initial variable examination
```{r}
#Read dataset
data <- read.csv("houseValueData.csv")

#Changed with water to factor based on documentation; this is a categorical variable rather than an integer
data$withWater <- as.factor(data$withWater) 

#Initial variable examination
str(data)
sum(is.na(data))
summary(data)
```

The provided dataset contains 400 observations of 11 variables, with no missing values. Below is a view of the histograms of all numeric variables in the dataset.

```{r}
#Histogram of variables 
ggplot(melt(data[,-3]), aes(value)) + geom_histogram(color = "black", fill = "white") + facet_wrap(~variable, scales = "free") + labs(title = "Histogram of Variables")
```

In order to get detailed summary statistics, we use the following function:
```{r}

#Detailed summary statistics function
ContStat = function(x,y) {
#x must be a vector, not a dataframe
#y is the number of decimal points to round data to
StatLen  = length(x)  
StatNA = sum(is.na(x))
StatMean = summary(x)["Mean"]
StatMin  = summary(x)["Min."]
StatMax= summary(x)["Max."]
StatSd  = sd(x)

StatQuan = quantile(x,c(0.01,0.05,0.1,0.25,0.5,0.75,0.9,0.95,0.99))

rownms =c("N", "#NA's","Mean","Min","Max","Std", "1%","5%","10%","25%","50%","75%","90%","95%","99%")

Stats = c(StatLen,StatNA,StatMean,StatMin,StatMax,StatSd, StatQuan)

ContStatDF = as.data.frame(Stats, row.names=rownms)
ContStatDF = round(ContStatDF,y)
return(ContStatDF)
}

```


In order to output histograms and scatterplots of each variable, we use the following function:
```{r}

#Histogram and Scatterplot of variables
Graphs = function(x, y) {
#vect must be a vector, not a dataframe
#y is a string, the name of the variable of interest.  Used for labeling the graphs
  
subdata = data[,c(x,'homeValue')]
names(subdata)[1] = 'variable'
    
hist =  ggplot(data=subdata, aes(variable)) + geom_histogram() + ggtitle("Histogram")+ scale_x_continuous("Bin") + scale_y_continuous("Count") 

sp = ggplot(data=subdata, aes(x=variable, y=homeValue)) +  geom_point(shape=16) +  ggtitle("Scatterplot")+ scale_x_continuous(y) + scale_y_continuous(name = "Home Value", labels = comma)    
  
output = grid.arrange(hist, sp, ncol=2,nrow=1, top = textGrob(paste("Histogram and Scatterplot of" , y , sep=" ") ,gp=gpar(fontsize=12,font=2)))
  
return(output)  
}

```

###Detailed Variable Examination

We will now take a more detailed examination of each of the variables and their relationship with our variable of interest: homeValue. 

------------------------------------------------------------------------
**HomeValue**
First, homeValue itself.  From the attached text file, homeValue is defined as _median price of single-family house in the neighborhood (measured in dollars)_.
------------------------------------------------------------------------

```{r}
#Examine HomeValue
ggplot(data=data, aes(data$homeValue)) + geom_histogram()
ContStat(data$homeValue,0)
```

The range of the variable is 112,500 through 1,125,000  There dont appear to be any values that are unreasonable for the homeValue variable.  The histogram shows a strong right skew of the variable with many of the values clustered together between the first and third quartile.  While this is the target variable of interest, I will also create a log (homeValue) price and use both of them to find the model with the best fit.

------------------------------------------------------------------------
**CrimeRate_pc**

Next lets take a look at the crimeRatepc variable which is defined as _crime rate per capita, measured by number of crimes per 1000 residents in neighborhood_.
------------------------------------------------------------------------

```{r}
#Examine CrimeRate_pc
Graphs('crimeRate_pc', 'Crime Rate')
ContStat(data$crimeRate_pc,2)
```

Crime rate per capita shows a slight negative correlation against Home Value.  However, there is an extremely large number of neighborhoods that have a crime rate of zero or close to zero.  The scatterplot shows that crime rate is more dispersed around areas of lower home value. That being said, there appears to be a small ceiling in the scatterplot- six points that all seem to have the same home value but with varying crime rates.  Lets take a closer look at those points.

```{r}
#Examine ceiling effect
subset(data, homeValue>1100000)
```

These points seem to indicate that there is a maximum limit on the homeValue. These values likely represent areas in which the median price is greater than 1125000. This means these data points are not likely to be continuous, and thus will be difficult to accurately predict these points as these observations could have a true median homeValue of 1125000 or even ten or fifty times that value. Having identified this ceiling, we should check to see if there is a floor for the minimum home value. 

```{r}
#Examine potential floor effect
subset(data, homeValue<126000)
```

With only one value at the minimum, it seems unlikely that there is a minimum limit to the home value.


------------------------------------------------------------------------
**NonRetailBusiness**

Next let's take a look at the nonRetailBusiness variable which is defined as _the proportion of non-retail business acres per neighborhood_.
------------------------------------------------------------------------


```{r}
#Examine nonRetailBusiness
Graphs('nonRetailBusiness', 'Non Retail Business')
ContStat(data$nonRetailBusiness,2)
```

The range for non retail business is 0.01 through 0.28.  There is a negative correlation between the percentage of non retail business and the home value.  While the data on the left side of the scatterplot seems to be random according to Non Retail Business, on the right side of the scatterplot, the values create a series of lines.  We will now take a deeper look and find the most common values for this variable. 

```{r}
#Frequencies of nonRetailBusiness
freqs = count(data$nonRetailBusiness)
freqs[with(freqs,order(-freq)),]
```

There are 104 records here (over 25%!) with the same value of 0.1810 for the percentage of non retail business.  This is a curious result.  Lets examine these records in detail. 

```{r}
#Subset data
subset(data, nonRetailBusiness==.181)
```

Not only do these records have the same value for Non Retail Business, but also for distance to highway and pupil teacher ratio.  This could indicate a problem because 25% of our records have the same value for 3 of 10 variables.  It is very likely that these three can be used together for any model due to multicolinearity. 

There was also a high number of records that had a value of 0.1958 for the non retail business variable.  Let's take a look at those as well. 

```{r}
#Subset data
subset(data, nonRetailBusiness==.1958)
```

The same issue as above with another 25 sharing the same values.

```{r}
#Subset data
subset(data, nonRetailBusiness==.0814)
```

The same issue with another 15 sharing the same values.  These 15 also have the same value for pollutionIndex. In fact, when nonRetailBusiness values of 0.0620, 0.2189, 0.0397, 0.0990, 0.1001 and 0.0856 are further examined, we notice that they all have the same values for  pollutionindex, distance to highway and pupil teacher ratio.  These variables will likely not contribute much together, as they tend to vary together as a group.

------------------------------------------------------------------------
**WithWater**
The next variable is withwater which is defined as  _whether the neighborhood is within 5 miles of a water body (lake, river, etc); 1 if true and 0 otherwise_
------------------------------------------------------------------------

As this is a binary variable, the functions created above are not appropriate.
```{r}
#Examine withWater
table(data$withWater)
ggplot(data, aes(withWater, homeValue)) + geom_boxplot() +  scale_y_continuous(name = "Home Value", labels = comma) + scale_x_discrete(name = "With Water: 1 = Yes, 0 = No")
```

With water tends to have a slightly higher median home value than without water. Neighborhoods without water do tend to see some higher home values, but these are considered outliers that fall outside of the upper whisker.

------------------------------------------------------------------------
**ageHouse**

Now we will examine ageHouse, which is defined as _proportion of houses built before 1950_
------------------------------------------------------------------------

```{r}
#Examine ageHouse
Graphs('ageHouse', 'Percentage of house built before 1950')
ContStat(data$ageHouse,1)
```

The range here is from 2.9 through 100.0 with a left skew indicating many of these neighborhoods have older homes (built before 1950). With such age buckets, there is ambiguity between neighborhoods built in 1950 and those in 1850. This might account for the larger variation in older neighborhoods home value. Even so, the newer neighborhoods seem to have higher home values, especially given that there is less of a spread than for older homes. Average age of the home would be a better variable in this case.

------------------------------------------------------------------------
**distanceToCity**

distanceToCity is next which is _distance to the nearest city (measured in miles)_
------------------------------------------------------------------------

```{r}
#Examine distanceToCity
Graphs('distanceToCity', 'Miles to Nearest House')
ContStat(data$distanceToCity ,1)
```

Interestingly, the minimum value here is not 0 which indicates that none of these neighborhoods are actually in the city. The histogram tends to be right skewed, indicating that many neighborhoods are close to the city, while a few are over 40 miles from the city.

------------------------------------------------------------------------
**distanceToHighway**

distanceToHighway is next which is _distances to the nearest highway (measured in miles)_
------------------------------------------------------------------------

```{r}
#Examine distance to highway
Graphs('distanceToHighway', 'Distance to highway in miles')
ContStat(data$distanceToHighway ,1)
```

Distance to highway has a mean of 9.6, yet the most frequent value is 24, which occurs over 100 times in the dataset. There doesn't seem to be a clear relationship between home value and distance to highway, especially given the gap in values between 9 and 24. As stated previously, this variable probably will not contribute much to predicting home value.

------------------------------------------------------------------------
**pupilTeacherRatio**

pupilTeacherRatio is next which is _average pupil-teacher ratio in all the schools in the neighborhood_
------------------------------------------------------------------------

```{r}
#Examine pupil teacher ratio
Graphs('pupilTeacherRatio', 'Pupil Teacher Ratio')
ContStat(data$pupilTeacherRatio ,1)

#Get mode of pupil teacher ratio
Mode <- function(x) {
  ux <- unique(x)
  ux[which.max(tabulate(match(x, ux)))]
}

Mode(data$pupilTeacherRatio)
```

Pupil teacher ratio has a mean of 21.4, but has a strikingly frequent amount at 23.2. As discussed previously, this tends to covary with two of the other variables in the dataset. There does seem to be a negative relationship between pupil teacher ratio and home value.

------------------------------------------------------------------------
**pctLowIncome**

The next variable is pctLowIncome which is _percentage of low income household in the neighborhood_
------------------------------------------------------------------------

```{r}
#Examine pctLowIncome
Graphs('pctLowIncome', 'Percent of low income households in the neighborhood')
ContStat(data$pctLowIncome ,1)
```

There is a very strong negative correlation on this scatterplot, unsurprisingly.  If you have a low income its unlikely that you can afford a house with a high value.  This variable is also right skewed, as demonstrated by the histogram. 

------------------------------------------------------------------------
**pollutionIndex**

The next variable is pollutionIndex which is defined as _scaled between 0 and 100, with 0 being the best and 100 being the worst (i.e. uninhabitable)._ Even though it is highly correlated with non retail business, distance to highway and pupil teacher ratio, we will investigate it because the philanthropist group is interested.
------------------------------------------------------------------------


```{r}
#Examine pollutionIndex
Graphs('pollutionIndex', 'Pollution Index')
ContStat(data$pollutionIndex ,1)
```

The scatterplot displays multiple segments: high home values and relativey low polution, medium home value and medium pollution, and low home value and high polution. There does seem to be a negative correlation between pollution index and home value, although the scatterplot shows a lot of variation. The histrogram shows a right skew.

------------------------------------------------------------------------
**nBedRooms**

The final variable is nBedRooms which is _the average number of bed rooms in the single family houses in the neighborhood_
------------------------------------------------------------------------

```{r}
Graphs('nBedRooms', 'Average Number of Bedrooms')
ContStat(data$nBedRooms ,1)
```

Finally! A normally distributed variable.  This one is also positively correlated with home value.  This will likely be one of the most useful of the prediction variables. It ranges from 1.6 to 6.4, which are reasonable average bedrooms for houses.


##Decisions based off data exploration:
From the original dataset, the following decisions were then made.

1. Eliminate the variables non retail business, distance to highway and student pupil ratio  as they have too much colinearity with each other. We suspect there may be some sampling error or additional information that we would have to ask the client for.
2. While pollutionindex is correlated with the three above, as the group specifically asked about it, it will be kept in the model for now.
3. Create a transformation of home value, log home value, that will be used for fitting the model.  Whichever outcome variable performs the best will be used.
4. No other transformations will be used at this time.  If the model fit is poor, then transformations will be considered.
5. For home value, there are 8 records that are categorical rather than continuous.  These are the values that likely mean 1125000 or greater.  Because we do not know the true value, we will not include them in our model.

----------------


First, we will subset the data and transform some of the variables:

```{r}
#Subset data to remove categorical home values (ceiling)
data = subset(data, homeValue!=1125000)

#Create log home value
data$loghomeValue = log(data$homeValue)

#Examine Transformation
ggplot(data=data, aes(data$loghomeValue)) + geom_histogram()
ContStat(data$loghomeValue ,1)
```

As expected, the log home value transformation has made the histogram more normal, although there is a tail to the left.

We also will create two new binary variables, crimeRate_zero which indicates a very low crime rate and older neighborhood which indicates if 100% of the houses was built before 1950.  Finally we have newerneighborhood which indicates if 25% or less of the houses were built before 1950.

```{r}

#Create Indicator Variables

data$crimeRate_zero[data$crimeRate_pc < 30.0] <- 1
data$crimeRate_zero[data$crimeRate_pc >= 30.0] <- 0
data$crimeRate_zero <-as.factor(data$crimeRate_zero)

data$olderneighborhood [data$ageHouse >= 100.00] <- 1
data$olderneighborhood [data$ageHouse < 100.00] <- 0
data$olderneighborhood <- as.factor(data$olderneighborhood )

#crimeRate_zero
table(data$crimeRate_zero)
ggplot(data, aes(crimeRate_zero, homeValue)) + geom_boxplot() +  scale_y_continuous(name = "Home Value", labels = comma) 

#olderneighborhood
table(data$olderneighborhood)
ggplot(data, aes(olderneighborhood, homeValue)) + geom_boxplot() +  scale_y_continuous(name = "Home Value", labels = comma) 

```

All of the transformations box plots look reasonable.

Now we will create two models using the variables identifed above.  One will have homevalue as the dependent variable while the other will have the log of home value.


```{r}
#Create Models

lm = lm(homeValue ~ crimeRate_pc+crimeRate_zero+olderneighborhood  +withWater+ageHouse+distanceToCity+pctLowIncome+pollutionIndex+nBedRooms, data=data)

lmlog = lm(loghomeValue ~ crimeRate_pc+crimeRate_zero+olderneighborhood  +withWater+ageHouse+distanceToCity+pctLowIncome+pollutionIndex+nBedRooms, data=data)

#Summarize Models
summary(lm)
summary(lmlog)
```

Let's remove the nonsignificant variables and take another look:

```{r}
#Create Models

lm = lm(homeValue ~ crimeRate_pc+withWater+olderneighborhood+distanceToCity+pctLowIncome+pollutionIndex+nBedRooms, data=data)

lmlog = lm(loghomeValue ~ crimeRate_pc+withWater+olderneighborhood+distanceToCity+pctLowIncome+pollutionIndex+nBedRooms, data=data)

#Summarize Models
summary(lm)
summary(lmlog)
```

Now everything in the model is significantly accounting for variance. Lets take a look at histograms of the residuals. 

```{r}
lmresid = ggplot(data=lm, aes(lm$residuals)) + geom_histogram() +  ggtitle("Histogram of Home Value Model Residuals")
lmlogresid =ggplot(data=lmlog, aes(lmlog$residuals)) + geom_histogram() +  ggtitle("Histogram of Log Home Value Model Residuals")
  
grid.arrange(lmresid, lmlogresid, ncol=2,nrow=1)

```

Both sets of residuals are fairly normal, although the log home value residuals are more normal.  That in addition to its higher r squared score makes it the favorite thus far.  However, lets take a look at the residual disagnostic plots for them before any final decision or addition or transformation of variables is undertaken.


```{r}
plot(lm)
plot(lmlog)
```

Both models show evidence of heteroscedasticity in their residuals vs fitted plots. We would want the residuals to be an even band with no obvious clustering or curvature. Clearly this is not the case. The log home value model is worse in this sense than the normal one. Both Q-Q plots show that the residuals are pretty normally distributed. We know this because they closely follow the straight line which would indicate a normal distribution. 

Both scale-location plots also indicate some heteroscedasticity. Again, if the errors were homoskedastic we would expect an even distribution of errors. There is both clustering and curvature indicated by the smoothing function. Finally, the leverage plot indicates that while there are points with a large amount of leverage, they are within our bounds.

There are several issues with these plots that suggest we do not perfectly meet the definition of the Classical Linear Model. However, we can say that we have met the asymptotic assumptions of linear regression. Generally asymptotic assumptions can be used on a sample that is greater than 30, which we clearly have met. We have already met the first three conditions, by having linear parameters, assuming the data came from a random sample, and showing no multicolinearity. Therefore, we will test for exogeneity. Exogeneity is defined as no correlation between a particular x variable and the error terms in our model. 

```{r}
#Test regular model
cov(data$crimeRate_pc, lm$residuals)
cov(data$distanceToCity, lm$residuals)
cov(data$pctLowIncome, lm$residuals)
cov(data$pollutionIndex, lm$residuals)
cov(data$nBedRooms, lm$residuals)

#Test log model
cov(data$crimeRate_pc, lmlog$residuals)
cov(data$distanceToCity, lmlog$residuals)
cov(data$pctLowIncome, lmlog$residuals)
cov(data$pollutionIndex, lmlog$residuals)
cov(data$nBedRooms, lmlog$residuals)
```

As all of these values are quite small, we believe it is reasonable to assume we have met exogeneity. This means that we can claim our model parameters are consistent, which means that the bias decreases as the number of observations increases. This means we are reasonably confident we can use these statistics to estimate our population parameters. 

At this point we need to either transform variables or add interaction terms.

From the original variable analysis, we know that crimerate_pc and pctLowIncome are skewed to the left.  Let's take a look at their distributions when square rooted.


```{r}
data$sqrtpctIncome = sqrt(data$pctLowIncome)
Graphs('sqrtpctIncome', 'Square Root of Percent of low income households in the neighborhood')
```


The histogram looks far more normal and the scatterplot was not affected negatively which is a great sign.

```{r}
data$sqrtcrimeRate_pc = sqrt(data$crimeRate_pc)
Graphs('sqrtcrimeRate_pc', 'Square Root of Crime Rate in the neighborhood')
```

With such a strong left skew, even the square root here does not make the data any more normal in the histogram.  However, model performance may have improved.
I also want to take another look at withWater as that was a variable of interest.

```{r}

lm = lm(homeValue ~ sqrtcrimeRate_pc+distanceToCity+olderneighborhood+sqrtpctIncome +pollutionIndex+nBedRooms+withWater, data=data)
lmlog = lm(loghomeValue ~ (crimeRate_pc+distanceToCity+olderneighborhood+sqrtpctIncome +pollutionIndex+nBedRooms+withWater), data=data)

lmlog = lm(loghomeValue ~ (crimeRate_pc+distanceToCity+olderneighborhood+pctLowIncome +pollutionIndex+nBedRooms+pctLowIncome*nBedRooms+withWater), data=data)

lmlog = lm(loghomeValue ~ (crimeRate_pc+distanceToCity+olderneighborhood+pctLowIncome +pollutionIndex+nBedRooms+pctLowIncome*nBedRooms+crimeRate_pc), data=data)
summary(lmlog)

#Summarize Models
summary(lm)
summary(lmlog)

plot(data$nBedRooms,data$pctLowIncome)
mean(data$pctLowIncome)
data$lowincome[data$pctLowIncome >= 16] <- 1
data$lowincome[data$pctLowIncome < 16] <- 0
mean(data$nBedRooms)
data$fewrooms[data$nBedRooms >= 4.2] <- 1
data$fewrooms[data$nBedRooms < 4.2] <- 0
data$fewrooms <-as.factor(data$fewrooms)

lows <- subset(data, lowincome==1)
plot(lows$nBedRooms,lows$homeValue)
highs <- subset(data, lowincome==0)
plot(highs$nBedRooms,highs$homeValue)
mean(highs$nBedRooms)
mean(lows$nBedRooms)

mytable <- table(data$homeValue, data$fewrooms, data$lowincome) 
aggregate(data$homeValue, by=list(data$fewrooms, data$lowincome), mean)
```

The fit indicated by the r squared value is slightly better. Let's take a look at the residual plots:

```{r}
plot(lm)
plot(lmlog)
print(data([n = 250]
data[c(250,378,278,72,99,95,213),]
hist(sqrt(data$pctLowIncome))


interactionMeans(lmlog)
plot(data$nBedRooms,data$pctLowIncome)
mean(data$pctLowIncome)
data$lowincome[data$pctLowIncome >= 16] <- 1
data$lowincome[data$pctLowIncome < 16] <- 0
mean(data$nBedRooms)
data$fewrooms[data$nBedRooms >= 4.2] <- 1
data$fewrooms[data$nBedRooms < 4.2] <- 0
data$fewrooms <-as.factor(data$fewrooms)

lows <- subset(data, lowincome==1)
plot(lows$nBedRooms,lows$homeValue)
highs <- subset(data, lowincome==0)
plot(highs$nBedRooms,highs$homeValue)
mean(highs$nBedRooms)
mean(lows$nBedRooms)

mytable <- table(data$homeValue, data$fewrooms, data$lowincome) 
aggregate(data$homeValue, by=list(data$fewrooms, data$lowincome), mean)

```


There still seems to be heteroscedasticity.  However, no amount of other transformation helped.  Other transformations that were considered included, rounding the number of bedrooms to the nearest whole number, taking the log of distancetoCity and other binary considerations.

The log home value model is the one we will select to answer the questions of the group.  However, due to the heteroscedasticity, we will need to ensure we are using robust standard errors.


```{r}
lmlog$newse<-vcovHC(lmlog)
coeftest(lmlog,lmlog$newse)
```

Specifically, the group wanted to know how environmental features affect the value of a home.  There are two variables in our model that address this, the binary withWater variable and the pollution index.

Because we are using a log scale for home Value, we have to interpret this as follows.

The neighborhood being within 5 miles of water increases the value of the home 8.3% versus not being in that proximitiy.

For every one unit increase in the pollutionIndex as it is calculated, the value of the home descreases by 0.5%.

The other variable that positively increases the value of the home is the number of bedrooms.  For each additional bedroom, the value of the home increases 10%.

#sqrt of home value#

##Question 2

_Build a time-series model for the series in series02.txt and use it to perform a 24-step ahead forecast._

```{r}
#Import data
series <- read.table("series02.txt")
series <- ts(series$V1)

#Plot data
par(mfrow = c(2,2))
plot.ts(series, col = "navy", xlab = "Time Period", ylab = "Values", main = "Time Series for Series 02")
hist(series, main = "Histogram of Values of Series 02")
acf(series, main = "ACF of Series 02")
pacf(series, main = "PACF of Series 02")
```

Notice the general structure of the series. There seems to be a long run average, where the values are fluctuating around a central axis but with with a major series of spikes in the beginning signaling serious volatility. There does not seem to be seasonality or a trend. The ACF interestingly shows a sharp drop after the 0 lag, but slightly statistically significant lags throughout the series. The PACF also shows slight significance at several lags after the most significant at what looks like the 3rd lag. 

We suspect there is non-constant variance present in this series, so we will plot a correlogram of the squared values of a mean adjusted version of this series (adjusted so the mean is zero). 

```{r}
#Autocorrelation Function
par(mfrow = c(1,1))
acf((series - mean(series))^2, main = "ACF of Squared Terms")
```

The square values that are plotted are equivalent to the variance. What the statistically significant values indicate is that there is serial correlation, meaning conditional heteroskedasticity. In plain English, this means that the variance is not constant throughout the series, rather the variance depends on what window of time we are looking at. This violates a core assumption of stationarity, meaning we will have to use a non-stationary model to fit this data. 

```{r}
garch.fit <- garchFit(~garch(1,1), data = series, trace = FALSE, include.mean = FALSE)
garch.fit

par(mfrow = c(2,1))
#Note standardized residuals because garchFit calculates residuals differently
acf(residuals(garch.fit, standardize = TRUE), main = "Residuals of Garch Model")
acf(residuals(garch.fit, standardize = TRUE)^2, main = "Residuals of Garch Model Squared")
Box.test(residuals(garch.fit, standardize = TRUE), type = "Ljung-Box")
```

Notice that with the ACF of both the series residuals and squared residuals there is no autocorrelation. This suggests the residuals are behaving like white noise and thus the model is a good fit. The residuals also fail to reject the null hypothesis that the residuals are independent. The coefficients are all statistically significant, meaning we reject the null hypothesis that the coefficients are 0. Therefore, we believe this model is a good fit for forecasting. 

```{r}
preds <- predict(garch.fit, n.ahead = 24)
lower <- preds$meanForecast - 1.96 * preds$meanError
upper <- preds$meanForecast + 1.96 * preds$meanError
cbind(lower, preds$meanForecast, upper)
```

We have printed the forecast above. The 0 predicted value should make sense, this is a model of volatility. We will plot the results below

```{r}
par(mfrow = c(1,1))
plot.ts(c(series, preds$meanForecast), main = "Forecast Plot")
lines(c(rep(NA, 792), lower), col = "blue")
lines(c(rep(NA, 792), upper), col = "blue")
```

The two blue lines represent the boundary of the predicted volatility. These seem to be in line with the long run average of the series. 

##Question 3

_Build a time-series model for the series in series03.csv and use it to perform a 24-step ahead forecast_

```{r}
#Load data
series <- read.csv("series03.csv")
series <- ts(series$X9.88)

#Plot data
par(mfrow = c(2,2))
plot.ts(series, xlab = "Time Period", ylab = "Value", main = "Time Series Plot of Series 03", col = "navy")
hist(series, main = "Histogram of Series 03")
acf(series, main = "ACF of Series 03")
pacf(series, main = "PACF of Series 03")
```

Notice from the time series plot that there is significant trend going on, specifically, a long term upward trend. The ACF shows significance through all past lags while the PACF is only siginficant for the first lag. There does not seem to be any seasonality. This looks like the realization of a random walk with drift process. 


```{r}
#Plot different time series to suggest differencing
par(mfrow = c(3, 2))
plot.ts(series, main = "Original Time Series")
plot.ts(log(series), main = "Log of Time Series")
plot.ts(diff(series), main = "First Difference of Time Series")
plot.ts(diff(series, d = 2), main = "Second Difference of Time Series")
plot.ts(diff(log(series)), main = "First Difference of Log Time Series")
plot.ts(diff(log(series), d = 2), main = "Second Difference of Log Time Series")
```

It is clear from the original time series plot that the series is not stationary. Before proceeding to build a model we must render the series as stationary. 

```{r}
#Transform the series for stationarity
par(mfrow = c(2,2))
acf(diff(series), main = "ACF of First order Difference")
pacf(diff(series), main = "PACF of First order Difference")
acf(diff(series, d= 2), main = "ACF of Second order Difference")
pacf(diff(series, d = 2), main = "PACF of Second order Difference")

acf(diff(log(series)), main = "ACF of First Order Difference of Log")
pacf(diff(log(series)), main = "PACF of First Order Difference of Log")
acf(diff(log(series), d = 2), main = "ACF of Second Order Difference of Log")
pacf(diff(log(series), d = 2), main = "PACF of Second Order Difference of Log")
```

From examining these plots, it seems as though the second order difference provides the best transformation into white noise. In both cases the ACF shows a sharp cut off (suggesting an MA term) while the PACF gradually declines. The first order difference shows a lot of volatility in the PACF, suggesting correlations that are not easily captured. 

Between the second order difference and the second order difference of the log, the second order difference of the log seems to look more like white noise. There are fewer significant autocorrelations (which might be due to sampling) in the second order difference of the log and it decays more smoothly. Therefore, we will use the second order difference of the log to estimate the model. 

```{r}

#Function to select the best ARIMA based on AIC
get.best.arima <- function(x.ts, maxord = c(1,1,1))
{
  best.aic <- 1e8
  n <- length(x.ts)
  for (p in 0:maxord[1]) for (d in 0:maxord[2]) for (q in 0:maxord[3])
  {
    fit <- arima(x.ts, order = c(p, d, q), method = "ML")
    fit.aic <- -2 * fit$loglik + (log(n) + 1) * length(fit$coef)
    if (fit.aic < best.aic)
    {
      best.aic <- fit.aic
      best.fit <- fit
      best.model <- c(p, d, q)
    }
  }
  list(best.aic, best.fit, best.model)
}

auto.arima(log(series), allowdrift = FALSE)
mod <- auto.arima(log(series), d = 2)
mod
t(confint(mod))
```

Here we try using the auto.arima() function to find the best model. When using the auto.arima() function it suggests the first order difference of the log series. However, we saw above that this was not the best model examining the ACF and PACF so we instead specificed the order of differencing to be 2. When doing this, the suggested model is an ARIMA(2, 2, 1) model. However, examining the confidence intervals, we find that the 2 AR terms contain 0 in their confidence interval. That means we will fail to reject the null hypothesis these coefficients are 0. The MA term however does not contain 0 in its confidence interval and therefore we can reject the null hypothesis. Therefore, we will construct an ARIMA(0, 2, 1) model. 


```{r}
#Fit ARIMA (0,2,1)
model <- arima(log(series), order = c(0, 2, 1))
model
t(confint(model))
```

0 is not contained in the confidence interval so this coefficient is statistically significant. 


```{r}
#Diagnostic plots of residuals
resids <- model$residuals
plot.ts(resids, main = "Residuals of ARIMA Model")
hist(resids, main = "Histogram of Residuals")
acf(resids, main = "ACF of Residuals")
pacf(resids, main = "PACF of Residuals")
```

These residual diagnostics suggest a reasonably good approximation of white noise. The ACF and PACF however do show quite a bit of volatility, so we will examine the squared residuals because we suspect there is non-constant variance. 

```{r}
#Plot residuals and squared residuals
par(mfrow = c(2,1))
acf(resids, main = "ACF of Residuals")
acf(resids^2, main = "ACF of Squared Residuals")
```

As we had suspected, the squared residuals show statistically significant terms at different intervals. Clearly, this suggests there is non-constant variance. Therefore, we will fit a GARCH model to the residuals. 

```{r}
#Fit GARCH model
garch.fit <- garchFit(~garch(1,1), data = resids, include.mean = FALSE, trace = FALSE)
garch.fit

par(mforw = c(2,1))
acf(residuals(garch.fit, standardize = TRUE),  main = "ACF of GARCH Residuals")
acf(residuals(garch.fit, standardize = TRUE)^2, main = "ACF of GARCH Residuals Squared")
Box.test(residuals(garch.fit, standardize = TRUE), type = "Ljung-Box")
```

The GARCH model shows statistically significant coefficients, meaning we will reject our null hypothesis that the coefficients are 0. Further, notice that the residuals now are not significant, meaning this series approximates white noise. The residuals also fail to reject the null hypothesis of the Ljung-Box test, meaning we cannot say the residuals are not independent. Therefore, we will use this model for forecasting. 

According to Cowpertwait, the fitted GARCH model on the residuals will not affect the average prediction, because the mean of residual errors is 0. However, it does affect the variance of predicted values. Therefore, we will use the ARIMA component of our model to provide point estimates for our forecast and the GARCH model to supply the standard error for the confidence interval. 

```{r}
preds <- forecast(model, h = 24)
std <- predict(garch.fit, n.ahead = 24)

#set confidence intervals
lower <- c(preds$mean - 1.96 * std$meanError)
upper <- c(preds$mean + 1.96 * std$meanError)
#display
cbind(lower, preds$mean, upper)

par(mfrow = c(1,1))
plot.ts(c(diff(log(series), d = 2), preds$mean), xlab = "Time Period", ylab = "Value", main = "Time Series Plot with Forecast")
```

#Still can't work out

##Question 4

_Build a time-series model for the series in series04.csv and use it to perform a 24-step ahead forecast. Possible models include AR, MA, ARMA, ARIMA, Seasonal ARIMA, GARCH, ARIMA-GARCH, or Seasonal ARIMA-GARCH models. Note that the original series may need to be transformed before it be modelled._

```{r}
#Import data
series <- read.csv("series04.csv")
series <- ts(series$X25182)

#Plot data
par(mfrow = c(2,2))
plot.ts(series, xlab = "Time Period", ylab = "Values", main = "Time Series Plot of Series 04")
hist(series, main = "Histogram of Series 04")
acf(series, main = "ACF of Series 04")
pacf(series, main = "PACF of Series 04")
```

From the time series plot it should be obvious that there is seasonality in this series, suggesting seasonal lag terms will be needed. The series shows a general upwards trend, and we would argue this series is definitely not stationary. The ACF show statistically significant lags persisting but at different heights, further suggesting non-stationarity and seasonality. 

```{r}
#Transform series for stationary
series <- read.csv("series04.csv")
series <- ts(series$X25182, frequency = 12)

plot.ts(diff(log(series)), main = "Time Series of Log First Difference")
hist(diff(log(series)), main = "Histogram of Log First Difference")
acf(diff(log(series)), main = "ACF of Log First Difference")
pacf(diff(log(series)), main = "PACF of Log First Difference")
```

We are reimporting the series and setting the frequency to 12. We suspect the seasonality occurs on a monthly basis and counted 24 trough to peak cycles, indicating a seasonal period of 12. As is generally good practice we will take the log of the series and take the first difference to render the series more stationary. 

The time series plot of the differenced series resembles white noise. However, the ACF shows regular significance suggesting seasonal terms will be needed there. The PACF also shows seasonality although somewhat less as it decreases eventually. Both plots also show significance at the first lag suggesting non-seasonal terms will also be needed. 

```{r}
#Function to find the best ARIMA model. Credit to Cowpertwait and Metcalfe.
get.best.arima.seas <- function(x.ts, maxord = c(1,1,1,1,1,1)) {
  best.aic <- 1e8
  n <- length(x.ts)
  for (p in 0:maxord[1]) for(d in 0:maxord[2]) for(q in 0:maxord[3])
    for (P in 0:maxord[4]) for(D in 0:maxord[5]) for(Q in maxord[6])
    {
      fit <- arima(x.ts, order = c(p, d, q), seas = list(order = c(P,D,Q), 12), method = "CSS")
      fit.aic <- -2 * fit$loglik + (log(n) + 1) * length(fit$coef)
      if (fit.aic < best.aic)
      {
        best.aic <- fit.aic
        best.fit <- fit
        best.model <- c(p, d, q, P, D, Q)
      }
    }
  list(best.aic, best.fit, best.model)
}

get.best.arima.seas(log(series), maxord = rep(3, 6))
auto.arima(log(series), d = 1, D = 1) #note specified the use of seasonality

#get.best -> (3, 0, 3) (2, 0, 3) [12]
#auto -> (2, 1, 0) (2, 1, 2) [12]

mod <- auto.arima(log(series), d = 1, D = 1)
mod2 <- arima(log(series), order = c(3, 0, 3), seasonal = list(order = c(2, 0, 3), 12))
mod
mod2
```

We utilized both the auto.arima() function and the get.best.arima.seas() function (from the time series textbook) to acquire suggested model fits. However, we know the model should include a first difference and a seasonal difference from our previous investigation. Otherwise the model will not be stationary, and we will be unable to fit a model to it. The auto.arima() model's AIC is slightly higher `r AIC(mod)` versus `r AIC(mod2)`, but we believe that the model suggested by auto arima will better satisfy our assumptions. Therefore, we will investigate this model going forward. 

```{r}
#Model comparisons
t(confint(mod))
#Base comparison model is (2, 1, 0)(2, 1, 2)[12] with AIC -779.5714
mod3 <- arima(log(series), order = c(3, 1, 0), seasonal = list(order = c(2, 1, 2), 12))
mod4 <- arima(log(series), order = c(2, 1, 1), seasonal = list(order = c(2, 1, 2), 12))
mod5 <- arima(log(series), order = c(2, 1, 0), seasonal = list(order = c(3, 1, 2), 12))
mod6 <- arima(log(series), order = c(2, 1, 0), seasonal = list(order = c(2, 1, 3), 12))
AIC(mod3)
AIC(mod4)
AIC(mod5)
AIC(mod6)
```

Note that 0 is not contained in the confidence intervals of any of the terms for our model, which is currently ARIMA(2, 1, 0)(2, 1, 2)[12]. This means that we reject the null hypothesis and conclude that the evidence supports the alternative hypothesis that our model coefficients are different from 0. Further, above we have deliberately attempted to overfit our data by providing additional parameters. In all cases the AIC increases, suggesting that these models do not do a better job of explaining our data simply. In general, one wants a model that minimizes the AIC. 

Therefore, we will continue with residual diagnostics for our chosen model:

```{r}
#Examine model residuals
par(mfrow = c(2,2))
resids <- mod$residuals
plot.ts(resids, main = "Time Series Plot of Residuals")
hist(resids, main = "Time Series Plot of Residuals")
acf(resids, main = "ACF Plot of Residuals")
pacf(resids, main = "ACF Plot of Residuals")

par(mfrow = c(2, 1))
acf(resids, main = "ACF of Residuals")
acf(resids^2, main = "ACF of Residuals^2")
Box.test(resids, type = "Ljung-Box")

```

Overall, the residuals appear to largely resemble white noise. The time series plot looks fairly like white noise, with no obvious patterns suggesting seasonality or a trend. There is one large spike, which we would need to know more about this data to properly try to account for. This was noted in the time series plot of the original data. 
The ACF shows one significant term at around 3/4 and the PACF shows two significant terms around the same area. However, there are no highly significant terms in the early part of the model (beyond the expected term of the ACF) and there is no repeating pattern of terms that are significant. Further, the residuals fail to reject the null hypothesis of the Ljung-Box test, meaning that the evidence suggests the observations are independent. 
There are some terms that are significant in the residual squared ACF, but none are highly significant and as there are only three, this does not represent a large enough number to suggest our residuals are behaving other than white noise. 

We will note here that the residuals do not perfectly resemble white noise. There are still several lags showing statistical significant, which is not what we would want to see. However, we believe that we have fit the best possible model with the available information that we have. We would like to know more about the data and sampling methods to be able to fit the most appropriate possible model. We do however believe that we have satistified the conditions of stationarity and residuals behaving as white noise sufficiently to be able to forecast. 

```{r}
preds <- forecast(mod, h = 24)

lower <- preds$mean - preds$lower[,2]
upper <- preds$mean + preds$upper[,2]

cbind(lower, preds$mean, upper)

plot(forecast(mod, h = 24))
```

The plot shows a clearly a wide spectrum. We could transform the series back by taking the exponent but we believe the overall structure is clearly captured in this model. There is quite a wide spectrum. 

#NOTES SECTION AND APPENDIX: 

#Check on Box test, is that what it says?
#probably can be ultimately deleted.

------------------

#I did the following to see if the issue was the variables or the records before I found the scope of the problem.  Tossing all the records is too much, but I didnt want to delete this yet.

With the identification of variables that seem to strongly correlate, I want to do a couple of scatterplot matrices.

I will start with the first four identified.

```{r}
data3 = data[,c("nonRetailBusiness","distanceToHighway","pupilTeacherRatio","pollutionIndex")]
ggpairs(data3)

```


Surprisingly, there is not a super strong correlation between the variables (except between Non Retail Business and pollutionindex).  Perhaps the problem is with those records then and not the variables themselves.  Another subset will be created will just those 144 records first identified and anoother scatterplot matrix created.


```{r}
data2 = data[,c("nonRetailBusiness","distanceToHighway","pupilTeacherRatio","pollutionIndex")]
data3 = subset(data2, nonRetailBusiness==.181|nonRetailBusiness==.1958|nonRetailBusiness==.0814 )
ggpairs(data3)

```

There is far too much colinearity with these records for comfort.  I want to examine these 144 records in a scatterplot matrix with the other variables selected.

```{r}
data2 = subset(data, nonRetailBusiness==.181|nonRetailBusiness==.1958|nonRetailBusiness==.0814 )
data3 = data2[,c("crimeRate_pc","ageHouse","distanceToCity","pctLowIncome","homeValue","nBedRooms")]
ggpairs(data3)

```


The matrix here causes me no concern.  I am still unsure of whether or records or the variables are the problem here, so I will use the other 256 records and do the same two matrices.

```{r}
data2 = data[,c("nonRetailBusiness","distanceToHighway","pupilTeacherRatio","pollutionIndex")]
data3 = subset(data2, nonRetailBusiness!=.181&nonRetailBusiness!=.1958&nonRetailBusiness!=.0814 )
ggpairs(data3)

```



```{r}
data2 = subset(data, nonRetailBusiness!=.181&nonRetailBusiness!=.1958&nonRetailBusiness!=.0814 )
data3 = data2[,c("crimeRate_pc","ageHouse","distanceToCity","pctLowIncome","homeValue","nBedRooms")]
ggpairs(data3)

```

After all the examination of the variables and records, I have decided that the problem is with those 144 records.  I will create a new subset of the remaining 256 and continue to use all variables.

```{r}
data = subset(data, nonRetailBusiness!=.181&nonRetailBusiness!=.1958&nonRetailBusiness!=.0814 )

```

-----------------
