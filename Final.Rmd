---
title: "271 Final"
author: "Glenn (Ted) Dunmire, Marlea Gwinn, Julian Phillips"
date: "December 7, 2015"
output: html_document
---

```{r}

#Set Directory

#Ted
#setwd("~/Documents/271 Final")

#Marlea
setwd("C://Users/gwina003/Downloads/Final")

#Julian
#data <- read.csv("//vivica/Documents/MIDS/W271/271-Final/houseValueData.csv")

#Load Relevant Libraries
library(ggplot2)
library(car)
library(reshape2)
library(grid)
library(astsa)
library(forecast)
library(quantmod)
library(fGarch)
library(tseries)
library(gridExtra)
library(scales)
library(plyr)
library(GGally)
```

##Question 1

_Analyze each of these variables (as well as a combination of them) very carefully and use them (or a subset of them) to build a model and test hypotheses to address the questions. Also address potential (statistical) issues that may be casued by omitted variables._
_The philanthropist group hires a think tank to examine the relationship between the house values and neighborhood characteristics. For instance, they are interested in the extent to which houses in neighbhorhood with desirable features command higher values. They are specifically interested in environmental features, such as proximity to water body (i.e. lake, river, or ocean) or air quality._

```{r}
#Read dataset
data <- read.csv("houseValueData.csv")
data$withWater <- as.factor(data$withWater) #changed to factor based on documentation
```

Let us first begin with some basic examination of the data to see what kinds of variables we have and what their distributions look like. We have turned the withWater variable into a factor based on the documentation, because it is a categorical variable rather than an int. 

```{r}
str(data)
sum(is.na(data))
summary(data)
```

Notice we have 400 observations of 11 variables, with no missing values. Here is a histogram with all variables in the same plot.

```{r}
ggplot(melt(data[,-3]), aes(value)) + geom_histogram(color = "black", fill = "white") + facet_wrap(~variable, scales = "free") + labs(title = "Histogram of Variables")
```

The following function will be used to get detailed summary statistics for each continuous variable:
```{r}

ContStat = function(x,y) {
#x must be a vector, not a dataframe
#y is the number of decimal points to round data to
StatLen  = length(x)  
StatNA = sum(is.na(x))
StatMean = summary(x)["Mean"]
StatMin  = summary(x)["Min."]
StatMax= summary(x)["Max."]
StatSd  = sd(x)


StatQuan = quantile(x,c(0.01,0.05,0.1,0.25,0.5,0.75,0.9,0.95,0.99))

rownms =c("N", "#NA's","Mean","Min","Max","Std", "1%","5%","10%","25%","50%","75%","90%","95%","99%")

Stats = c(StatLen,StatNA,StatMean,StatMin,StatMax,StatSd, StatQuan)

ContStatDF = as.data.frame(Stats, row.names=rownms)
ContStatDF = round(ContStatDF,y)
return(ContStatDF)
}

```


The following function will be used to output a histogram and a scatterplot:

```{r}

Graphs = function(x, y) {
#vect must be a vector, not a dataframe
#y is a string, the name of the variable of interest.  Used for labeling the graphs
  
subdata = data[,c(x,'homeValue')]
names(subdata)[1] = 'variable'
    
hist =  ggplot(data=subdata, aes(variable)) + geom_histogram() + ggtitle("Histogram")+ scale_x_continuous("Bin") + scale_y_continuous("Count") 

sp = ggplot(data=subdata, aes(x=variable, y=homeValue)) +  geom_point(shape=16) +  ggtitle("Scatterplot")+ scale_x_continuous(y) + scale_y_continuous(name = "Home Value", labels = comma)    
  
output = grid.arrange(hist, sp, ncol=2,nrow=1, top = textGrob(paste("Histogram and Scatterplot of" , y , sep=" ") ,gp=gpar(fontsize=12,font=2)))
  
return(output)  
}

```

Now let's take a closer look at each of the variables and its relationship with the variable of interest, homeValue.

First, homeValue itself.  From the attached text file, homeValue is defined as _median price of single-family house in the neighborhood (measured in dollars)_.

```{r}
ggplot(data=data, aes(data$homeValue)) + geom_histogram()
ContStat(data$homeValue,0)
```


 The range of the variable is 112,500 through 1,125,000  There dont appear to be any values that are unreasonable for the homeValue variable.  The histogram shows a strong right skew of the variable with many of the values clustered together between the first and third quartile.  While this is the target variable of interest, I will also create a log (homeValue) price and use both of them to find the model with the best fit.

 -----------------

Next lets take a look at the crimeRatepc variable which is defined as _crime rate per capita, measured by number of crimes per 1000 residents in neighborhood_.

```{r}
Graphs('crimeRate_pc', 'Crime Rate')
ContStat(data$crimeRate_pc,2)
```

Crime rate per capita shows a slight negative correlation against Home Value.  However, there is an extremely large number of neighborhoods that have a crime rate of zero or close to zero.  The scatterplot shows that crime rate is more dispersed around areas of lower home value. That being said, there appears to be a small ceiling in the scatterplot- six points that all seem to have the same home value but with varying crime rates.  Lets take a closer look at those points.

```{r}
subset(data, homeValue>1100000)
```


Unexpectedly there seems to be a maximum limit on the homeValue.  These values probably represent that median price being greater than 1125000 which means that the model will not be able to accurately predict points that great since these rows could have a true median homeValue of 1125000 or even ten or fifty times that value. Having identified this ceiling, we should check to see if there is a floor for the minimum home value. 

```{r}
subset(data, homeValue<126000)
```

With only one value at the minimum, it seems unlikely that there is a minimum limit to the home value.

A transformation for crime rate may be desired later, but all the other variables will be examined first.  Additionally, a decision on the steps to take with the maximum home value will be decided at the end of the variable examination.

---------------

Next let's take a look at the nonRetailBusiness variable which is defined as _the proportion of non-retail business acres per neighborhood_.

```{r}
Graphs('nonRetailBusiness', 'Non Retail Business')
ContStat(data$nonRetailBusiness,2)
```

The range for non retail business is 0.01 through 0.28.  There is also a negative correlation between the percentage of non retail business and the home value.  While the data on the left side of the scatterplot seems to be random according to Non Retail Business, on the right side of the scatterplot, they are lining up.  Let's take a deeper look.  Let's find the most common values for this variable.

```{r}
freqs = count(data$nonRetailBusiness)
freqs[with(freqs,order(-freq)),]
```

There are 104 records here (over 25%!) with the same value of 0.1810 for the percentage of non retail business.  This is a curious result.  Lets examine these records in detail. 


```{r}
subset(data, nonRetailBusiness==.181)
```


Not only do these records have the same value for Non Retail Business, but also for distance to highway and pupil teacher ratio.  This could indicate a probelem because 25% of our records have the same value for 3 of 10 variables.  It is very likely that these three can be used together for any model.

There was also a high number of records that had a value of 0.1958 for the non retail business variable.  Let's take a look at those as well. 

```{r}
subset(data, nonRetailBusiness==.1958)
```

The same issue as above with another 25 sharing the same values.

```{r}
subset(data, nonRetailBusiness==.0814)
```

The same issue with another 15 sharing the same values.  These 15 also have the same value for pollutionIndex.  Looking back, the 25 that had a nonRetailBusiness value of .1958 have only two different values.  The original 104 have different values, but I am now wary of a fourth variable.

In fact, when nonRetailBusiness values of 0.0620, 0.2189, 0.0397, 0.0990, 0.1001 and 0.0856 are also looked at, they all have the same values for  pollutionindex, distance to highway and pupil teacher ratio.  These variables will likely not contribute much together, as they tend to vary together as a group.

---------------

The next variable is withwater which is defined as  _whether the neighborhood is within 5 miles of a water body (lake, river, etc); 1 if true and 0 otherwise_

As this is a binary variable, the functions created above are not appropriate.
```{r}
table(data$withWater)
ggplot(data, aes(withWater, homeValue)) + geom_boxplot() +  scale_y_continuous(name = "Home Value", labels = comma) + scale_x_discrete(name = "With Water: 1 = Yes, 0 = No")
```

With water tends to have a slightly higher median home value than without water. Neighborhoods without water do tend to see some higher home values, but these are considered outliers that fall outside of the upper whisker.

----------------

Now we will examine ageHouse, which is defined as _proportion of houses built before 1950_

```{r}
Graphs('ageHouse', 'Percentage of house built before 1950')
ContStat(data$ageHouse,1)
```

The range here is from 2.9 through 100.0 with a left skew indicating many of these neighborhoods have older homes (built before 1950). With such age buckets, there is ambiguity between neighborhoods built in 1950 and those in 1850. This might account for the larger variation in older neighborhoods home value. Even so, the newer home neighborhoods seem to have higher home values given that there is less of a spread than for older homes. Average age of the home would be a better variable in this case.

----------------

distanceToCity is next which is _distance to the nearest city (measured in miles)_

```{r}
Graphs('distanceToCity', 'Miles to Nearest House')
ContStat(data$distanceToCity ,1)
```

Interestingly, the min value here is not 0 which indicates that none of these neighborhoods are actually in the city. The histogram tends to be right skewed, indicating that many neighborhoods are close to the city, while a few are over 40 miles from the city.

-----------------

The next two variables in the dataset are distanceToHighway and pupilTeacherRatio.  However, as discussed above these two variables will not be used in the model, so further investigation of them is unnecessary.

#I think we should probably talk about these... I can write them up tomorrow 

The next variable is another percent, pctLowIncome which is _percentage of low income household in the neighborhood_

```{r}
Graphs('pctLowIncome', 'Percent of low income households in the neighborhood')
ContStat(data$pctLowIncome ,1)
```

There is a very strong negative correlation on this scatterplot, unsurprisingly.  If you have a low income its unlikely that you can afford a house with a high value.  This variable is also right skewed, as demonstrated by the histogram. 

----------------

The next variable is pollutionIndex.  Even though it is highly correlated with non retail business, distance to highway and pupil teacher ratio, we will investigate it because the philanthropist group is interested.

Pollutionindex is defined as _scaled between 0 and 100, with 0 being the best and 100 being the worst (i.e. uninhabitable)_

```{r}
Graphs('pollutionIndex', 'Pollution Index')
ContStat(data$pollutionIndex ,1)
```

The scatterplot displays multiple segments: high home values and relativey low polution, medium home value and medium pollution, and low home value and high polution. There does seem to be a negative correlation between pollution index and home value, although the scatterplot shows a lot of variation. The histrogram shows a right skew.

----------------

The final variable is nBedRooms which is _the average number of bed rooms in the single family houses in the neighborhood_

```{r}
Graphs('nBedRooms', 'Average Number of Bedrooms')
ContStat(data$nBedRooms ,1)
```

Finally! A normally distributed variable.  This one is also positively correlated with home value.  This will likely be one of the most useful of the prediction variables. It ranges from 1.6 to 6.4, which are reasonable average bedrooms for houses.

----------------


From the original dataset, the following decisions were then made.

1. Eliminate the variables non retail business, distance to highway and student pupil ratio  as they have too much colinearity with each other.  
2. While pollutionindex is correlated with the three above, as the group specifically asked about it, it will be kept in the model for now.
3. Create a transformation of home value, log home value, that will be used for fitting the model.  Whichever outcome variable performs the best will be used
4. No other transformations will be used at this time.  If the model fit is poor, then transformations will be considered.
5. For home value, there are 8 records that are categorical rather than continuous.  These are the values that likely mean 1125000 or greater.  Because we dont know their true value at all, they will be discarded.

----------------

First, we will subset the data and transform some of the variables:

```{r}
#Subset data to remove categorical home values (ceiling)
data = subset(data, homeValue!=1125000)

#Transform Variables
data$loghomeValue = log(data$homeValue)
data$crimeRate_zero[data$crimeRate_pc < 1.0] <- 1
data$crimeRate_zero[data$crimeRate_pc >= 1.0] <- 0
data$crimeRate_zero <-as.factor(data$crimeRate_zero)
data$olderthan100[data$ageHouse >= 100.00] <- 1
data$olderthan100[data$ageHouse < 100.00] <- 0
data$olderthan100<- as.factor(data$olderthan100)


#Examine Transformations
ggplot(data=data, aes(data$loghomeValue)) + geom_histogram()
ContStat(data$loghomeValue ,1)

par(mfrow = c(2,2))
hist(data$pctLowIncome)
hist(sqrt(data$pctLowIncome))
hist(data$crimeRate_pc[data$crimeRate_pc >= 1.0], breaks = 30)
hist(sqrt(data$crimeRate_pc[data$crimeRate_pc >= 1.0]), breaks = 30)

```


As expected, the transformation has made the histogram more normal, although there is a tail to the left.

Now we will create two models using the variables identifed above.  One will have homevalue as the dependent variable while the other will have the log of home value.

```{r}
#Create Models

lm = lm(homeValue ~ crimeRate_pc+crimeRate_zero+olderthan100+withWater+ageHouse+distanceToCity+pctLowIncome+pollutionIndex+nBedRooms, data=data)

#removed agehouse, got to .77 R squared
lmlog = lm(loghomeValue ~ sqrt(crimeRate_pc)+crimeRate_zero+olderthan100+withWater+distanceToCity+sqrt(pctLowIncome)+pollutionIndex+nBedRooms, data=data)

#Summarize Models
summary(lm)
summary(lmlog)
```

In both models, withwater and ageHouse are far less significant than the other 5 variables.  I will remove them and recreate the models.

```{r}

lm = lm(homeValue ~ crimeRate_pc+olderthan100+withWater+distanceToCity+pctLowIncome+pollutionIndex+nBedRooms, data=data)
lmlog = lm(loghomeValue ~ crimeRate_pc+olderthan100+withWater+distanceToCity+sqrt(pctLowIncome)+pollutionIndex+nBedRooms, data=data)

#Summarize Models
summary(lm)
summary(lmlog)
```

The r squared values did not change which is unsurprising since the two variables removed were not adding a whole lot of new information to the model.  Lets take a look at histograms of the residuals. 

```{r}
lmresid = ggplot(data=lm, aes(lm$residuals)) + geom_histogram() +  ggtitle("Histogram of Home Value Model Residuals")
lmlogresid =ggplot(data=lmlog, aes(lmlog$residuals)) + geom_histogram() +  ggtitle("Histogram of Log Home Value Model Residuals")
  
grid.arrange(lmresid, lmlogresid, ncol=2,nrow=1)

```

Both sets of residuals are fairly normal, although the log home value residuals are more normal.  That in addition to its higher r squared score makes it the favorite thus far.  However, lets take a look at the residual disagnostic plots for them before any final decision or addition or transformation of variables is undertaken.


```{r}
plot(lm)
plot(lmlog)
```

Both models show evidence of heteroscedasticity in their residuals vs fitted plots.  The log home value model is worse in this sense than the normal one. Both Q-Q plots show that the residuals are pretty normally distributed.

Both scale-location plots also indicate some heteroscedasticity.  Finally, the leverage plot indicates that while there are points with a large amount of leverage, they are within our bounds.

At this point we need to either transform variables or add interaction terms.





##Question 2

_Build a time-series model for the series in series02.txt and use it to perform a 24-step ahead forecast._

```{r}
series <- read.table("series02.txt")
series <- ts(series$V1)

#Visualize the data
par(mfrow = c(2,2))
plot.ts(series, col = "navy", xlab = "Time Period", ylab = "Values", main = "Time Series for Series 02")
hist(series, main = "Histogram of Values of Series 02")
acf(series, main = "ACF of Series 02")
pacf(series, main = "PACF of Series 02")
```

Notice the general structure of the series. There seems to be a long run average, where the values are fluctuating around a central axis but with with a major series of spikes in the beginning signaling serious volatility. There does not seem to be seasonality or a trend. The ACF interestingly shows a sharp drop after the 0 lag, but slightly statistically significant lags throughout the series. The PACF also shows slight significance at several lags after the most significant at what looks like the 3rd lag. 

We suspect there is non-constant variance present in this series, so we will plot a correlogram of the squared values of a mean adjusted version of this series (adjusted so the mean is zero). 

```{r}
par(mfrow = c(1,1))
acf((series - mean(series))^2, main = "ACF of Squared Terms")
```

The square values that are plotted are equivalent to the variance. What the statistically significant values indicate is that there is serial correlation, meaning conditional heteroskedasticity. In plain English, this means that the variance is not constant throughout the series, rather the variance depends on what window of time we are looking at. This violates a core assumption of stationarity, meaning we will have to use a non-stationary model to fit this data. 

```{r}
garch1 <- garch(series, trace = F)
resids <- garch1$residuals[-1] #first value is NA

par(mfrow = c(2,1))
acf(resids, main = "ACF of GARCH(1,1) Residuals")
acf(resids^2, main = "ACF of GARCH(1,1) Residuals^2")
Box.test(resids, type = "Ljung-Box")
garch1
t(confint(garch1))
```

Notice that with the ACF of both the series residuals and squared residuals there is no autocorrelation. This suggests the residuals are behaving like white noise and thus the model is a good fit. Examining the model itself 0 is not contained in the 95% confidence intervals for the coefficients, meaning that the coefficients are satatistically significant at the 95% level. 

#Question: Do we think that's good enough? Look into fitting GARCH better
#Forecasting

##Question 3

_Build a time-series model for the series in series03.csv and use it to perform a 24-step ahead forecast_

```{r}
#load data and do preliminary visualization
series <- read.csv("series03.csv")
series <- ts(series$V1)

par(mfrow = c(2,2))
plot.ts(series, xlab = "Time Period", ylab = "Value", main = "Time Series Plot of Series 03", col = "navy")
hist(series, main = "Histogram of Series 03")
acf(series, main = "ACF of Series 03")
pacf(series, main = "PACF of Series 03")
```

Notice from the time series plot that there is significant trend going on, long term upwards. The ACF shows significance through all past lags while the PACF is only siginficant for the first lag. There does not seem to be any seasonality. This looks like the realization of a random walk with drift process. 


```{r}
#plot different time series to suggest differencing
par(mfrow = c(3, 2))
plot.ts(series, main = "Original Time Series")
plot.ts(log(series), main = "Log of Time Series")
plot.ts(diff(series), main = "First Difference of Time Series")
plot.ts(diff(series, d = 2), main = "Second Difference of Time Series")
plot.ts(diff(log(series)), main = "First Difference of Log Time Series")
plot.ts(diff(log(series), d = 2), main = "Second Difference of Log Time Series")
```

It is clear from the original time series plot that the series is not stationary. Before proceeding to build a model we must render the series as stationary. 

```{r}
par(mfrow = c(2,2))
acf(diff(series), main = "ACF of First order Difference")
pacf(diff(series), main = "PACF of First order Difference")
acf(diff(series, d= 2), main = "ACF of Second order Difference")
pacf(diff(series, d = 2), main = "PACF of Second order Difference")

acf(diff(log(series)), main = "ACF of First Order Difference of Log")
pacf(diff(log(series)), main = "PACF of First Order Difference of Log")
acf(diff(log(series), d = 2), main = "ACF of Second Order Difference of Log")
pacf(diff(log(series), d = 2), main = "PACF of Second Order Difference of Log")
```

From examining these plots, it seems as though the second order difference provides the best transformation into white noise. In both cases the ACF shows a sharp cut off (suggesting an MA term) while the PACF gradually declines. The first order difference shows a lot of volatility in the PACF, suggesting correlations that are not easily captured. 
Between the second order difference and the second order difference of the log, the second order difference of the log seems to look more like white noise. There are fewer significant autocorrelations (which might be due to sampling) in the second order difference of the log and it decays more smoothly. Therefore, we will use the second order difference of the log to estimate the model. 

```{r}
get.best.arima <- function(x.ts, maxord = c(1,1,1))
{
  best.aic <- 1e8
  n <- length(x.ts)
  for (p in 0:maxord[1]) for (d in 0:maxord[2]) for (q in 0:maxord[3])
  {
    fit <- arima(x.ts, order = c(p, d, q), method = "ML")
    fit.aic <- -2 * fit$loglik + (log(n) + 1) * length(fit$coef)
    if (fit.aic < best.aic)
    {
      best.aic <- fit.aic
      best.fit <- fit
      best.model <- c(p, d, q)
    }
  }
  list(best.aic, best.fit, best.model)
}

auto.arima(log(series), allowdrift = FALSE)
mod <- auto.arima(log(series), d = 2)
mod
t(confint(mod))
```

Here we try using the auto.arima() function to find the best best. When using the auto.arima() function it suggests the first order difference of the log series. However, we saw above that this was not the best model examining the ACF and PACF so we instead specificed the order of differencing to be 2. When doing this, the suggested model is an ARIMA(2, 2, 1) model. However, examining the confidence intervals, we find that the 2 AR terms contain 0 in their confidence interval. That means we will fail to reject the null hypothesis these coefficients are 0. The MA term however does not contain 0 in its confidence interval and therefore we can reject the null hypothesis. Therefore, we will construct an ARIMA(0, 2, 1) model. 

```{r}
model <- arima(log(series), order = c(0, 2, 1))
model
t(confint(model))
```

0 is not contained in the confidence interval so this coefficient is statistically significant. 


```{r}
#diagnostic plots of residuals
resids <- model$residuals
plot.ts(resids, main = "Residuals of ARIMA Model")
hist(resids, main = "Histogram of Residuals")
acf(resids, main = "ACF of Residuals")
pacf(resids, main = "PACF of Residuals")
```

These residual diagnostics suggest a reasonably good approximation of white noise. The ACF and PACF however do show quite a bit of volatility, so we will examine the squared residuals because we suspect there is non-constant variance. 

```{r}
par(mfrow = c(2,1))
acf(resids, main = "ACF of Residuals")
acf(resids^2, main = "ACF of Squared Residuals")
```

As we had suspected, the squared residuals show statistically significant terms at different intervals. Clearly, this suggests there is non-constant variance. Therefore, we will fit a GARCH model to the residuals. 

```{r}
resid.garch = garch(resids, trace = FALSE)
t(confint(resid.garch))
acf(resid.garch$residuals[-1], main = "ACF of GARCH fitted Residuals")
acf(resid.garch$residuals[-1]^2, main = "ACF of GARCH fitted Residuals^2")
```


The GARCH model shows statistically significant coefficients at the 95% level, because 0 is not contained in the confidence intervals. The ACFs of both the GARCH residuals and residuals squared show no significant values, meaning a good fit to the residuals. 

#Forecast?



##Question 4

_Build a time-series model for the series in series04.csv and use it to perform a 24-step ahead forecast. Possible models include AR, MA, ARMA, ARIMA, Seasonal ARIMA, GARCH, ARIMA-GARCH, or Seasonal ARIMA-GARCH models. Note that the original series may need to be transformed before it be modelled._

```{r}
#import data and run basic visualizations
series <- read.csv("series04.csv")
series <- ts(series$X25182)

par(mfrow = c(2,2))
plot.ts(series, xlab = "Time Period", ylab = "Values", main = "Time Series Plot of Series 04")
hist(series, main = "Histogram of Series 04")
acf(series, main = "ACF of Series 04")
pacf(series, main = "PACF of Series 04")
```

From the time series plot it should be obvious that there is seasonality in this series, suggesting seasonal lag terms will be needed. The series shows a general upwards trend, and we would argue this series is definitely not stationary. The ACF show statistically significant lags persisting but at different heights, further suggesting non-stationarity and seasonality. 

```{r}
par(mfrow = c(3,2))
plot.ts(series, xlab = "Time Period", ylab = "Values", main = "Time Series of Series 04 Original")
plot.ts(log(series), xlab = "Time Period", ylab = "Values", main = "Log of Series 04")
plot.ts(diff(series), xlab = "Time Period", ylab = "Values", main = "First Difference Series 04")
plot.ts(diff(log(series)), xlab = "Time Period", ylab = "Values", main = "First Difference Log Series 04")
plot.ts(diff(series, d = 2), xlab = "Time Period", ylab = "Values", main = "Second Difference Series 04")
plot.ts(diff(log(series), d =2), xlab = "Time Period", ylab = "Values", main = "Second Difference Log Series 04")
```

We are going to take a look at taking the log transformation and differencing of the series. What we want is ideally for the series to look like a white noise series (white noise by definition is stationary). At a glance, it appears the First Difference of the log of the series is a good candidate, it does not show varying levels of volatility through the series instead remaining largely uniform. Notice for example the first difference of the regular series shows changing volatility moving from left to right, characterized by the series fluctuating up and down more. This would suggest non-constant variance and thus the series would not be considered stationary. 

```{r}
#Examine ACF and PACF of First difference
par(mfrow = c(2,2))
acf(diff(series), main = "ACF of First Difference Series")
acf(diff(log(series)), main = "ACF of First Difference Log")
pacf(diff(series), main = "PACF of First Difference")
pacf(diff(log(series)), main = "PACF of First Difference Log")
```

Both the ACF and PACF of the first difference show highly significant lags at intervals of 6, suggesting a seasonal term is needed to capture this dynamic. This supports our earlier observation that there was seasonality present in this series. In particular the persistence of the lag in the ACF suggests a seasonal MA term at lag 6 would probably be appropriate. 

```{r}
series2 <- diff(log(series), lag = 6)
series3 <- diff(log(series), lag = 12)
acf(series2, main = "ACF of First Difference with Seasonality 6")
acf(series3, main = "ACF of First Difference with Seasonality 12")
pacf(series2, main = "PACF of First Difference with Seasonality 6")
pacf(series3, main = "PACF of First Difference with Seasonality 12")
```

Based on this it looks like there is seasonality at a period of 12 present in the data, which we would suggest probably comes from a monthly data set. 

```{r}
#function to find the best arima model. Credit to Cowpertwait and Metcalfe
get.best.arima.seas <- function(x.ts, maxord = c(1,1,1,1,1,1)) {
  best.aic <- 1e8
  n <- length(x.ts)
  for (p in 0:maxord[1]) for(d in 0:maxord[2]) for(q in 0:maxord[3])
    for (P in 0:maxord[4]) for(D in 0:maxord[5]) for(Q in maxord[6])
    {
      fit <- arima(x.ts, order = c(p, d, q), seas = list(order = c(P,D,Q), frequency(x.ts)), method = "CSS")
      fit.aic <- -2 * fit$loglik + (log(n) + 1) * length(fit$coef)
      if (fit.aic < best.aic)
      {
        best.aic <- fit.aic
        best.fit <- fit
        best.model <- c(p, d, q, P, D, Q)
      }
    }
  list(best.aic, best.fit, best.model)
}

get.best.arima.seas(log(series), maxord = rep(2, 6))
auto.arima(log(series))

#Which model?
#possible use of frequency = 12
(1,0,1)(0,1,1)[12] 
```








#Appendix

#probably can be ultimately deleted.

------------------

#I did the following to see if the issue was the variables or the records before I found the scope of the problem.  Tossing all the records is too much, but I didnt want to delete this yet.

With the identification of variables that seem to strongly correlate, I want to do a couple of scatterplot matrices.

I will start with the first four identified.

```{r}
data3 = data[,c("nonRetailBusiness","distanceToHighway","pupilTeacherRatio","pollutionIndex")]
ggpairs(data3)

```


Surprisingly, there is not a super strong correlation between the variables (except between Non Retail Business and pollutionindex).  Perhaps the problem is with those records then and not the variables themselves.  Another subset will be created will just those 144 records first identified and anoother scatterplot matrix created.


```{r}
data2 = data[,c("nonRetailBusiness","distanceToHighway","pupilTeacherRatio","pollutionIndex")]
data3 = subset(data2, nonRetailBusiness==.181|nonRetailBusiness==.1958|nonRetailBusiness==.0814 )
ggpairs(data3)

```

There is far too much colinearity with these records for comfort.  I want to examine these 144 records in a scatterplot matrix with the other variables selected.

```{r}
data2 = subset(data, nonRetailBusiness==.181|nonRetailBusiness==.1958|nonRetailBusiness==.0814 )
data3 = data2[,c("crimeRate_pc","ageHouse","distanceToCity","pctLowIncome","homeValue","nBedRooms")]
ggpairs(data3)

```


The matrix here causes me no concern.  I am still unsure of whether or records or the variables are the problem here, so I will use the other 256 records and do the same two matrices.

```{r}
data2 = data[,c("nonRetailBusiness","distanceToHighway","pupilTeacherRatio","pollutionIndex")]
data3 = subset(data2, nonRetailBusiness!=.181&nonRetailBusiness!=.1958&nonRetailBusiness!=.0814 )
ggpairs(data3)

```



```{r}
data2 = subset(data, nonRetailBusiness!=.181&nonRetailBusiness!=.1958&nonRetailBusiness!=.0814 )
data3 = data2[,c("crimeRate_pc","ageHouse","distanceToCity","pctLowIncome","homeValue","nBedRooms")]
ggpairs(data3)

```

After all the examination of the variables and records, I have decided that the problem is with those 144 records.  I will create a new subset of the remaining 256 and continue to use all variables.

```{r}
data = subset(data, nonRetailBusiness!=.181&nonRetailBusiness!=.1958&nonRetailBusiness!=.0814 )

```

-----------------
