---
title: "271 Final"
author: "Glenn (Ted) Dunmire"
date: "December 7, 2015"
output: html_document
---

```{r}
setwd("~/Documents/271 Final")
library(ggplot2)
library(reshape2)
library(grid)
library(astsa)
library(forecast)
library(quantmod)
library(fGarch)
library(tseries)
```

##Question 1

_Analyze each of these variables (as well as a combination of them) very carefully and use them (or a subset of them) to build a model and test hypotheses to address the questions. Also address potential (statistical) issues that may be casued by omitted variables._
_The philanthropist group hires a think tank to examine the relationship between the house values and neighborhood characteristics. For instance, they are interested in the extent to which houses in neighbhorhood with desirable features command higher values. They are specifically interested in environmental features, such as proximity to water body (i.e. lake, river, or ocean) or air quality._

```{r}
data <- read.csv("houseValueData.csv")
data$withWater <- as.factor(data$withWater) #changed to factor based on documentation
```

Let us first begin with some basic examination of the data to see what kinds of variables we have and what their distributions look like. We have turned the withWater variable into a factor based on the documentation, because it is a categorical variable rather than an int. 

```{r}
str(data)
sum(is.na(data))
summary(data)
```

Notice we have 400 observations of 11 variables, with no missing values. 

```{r}
ggplot(melt(data[,-3]), aes(value)) + geom_histogram(color = "black", fill = "white") + facet_wrap(~variable, scales = "free") + labs(title = "Histogram of Variables")

table(data$withWater)
```

```{r}
#Scatterplots of variables against log of home Value
data2 <- data
data2$logHomeValue <- log(data2$homeValue) #Log transform homeValue

plot_scatterplot = function(pl) {
  return(pl + geom_point(size=1.5) + 
           geom_smooth(method="lm", alpha=0.2) +
           theme(legend.key.size=unit(.3, "cm")))
}

crimePC.value.plot <- ggplot(data2, aes(crimeRate_pc, logHomeValue))
crimePC.value.plot = plot_scatterplot(crimePC.value.plot)

nonRetail.value.plot <- ggplot(data2, aes(nonRetailBusiness, logHomeValue))
nonRetail.value.plot = plot_scatterplot(nonRetail.value.plot)

ageHouse.value.plot <- ggplot(data2, aes(ageHouse, logHomeValue))
ageHouse.value.plot = plot_scatterplot(ageHouse.value.plot)

distanceCity.value.plot <- ggplot(data2, aes(distanceToCity, logHomeValue))
distanceCity.value.plot = plot_scatterplot(distanceCity.value.plot)

distanceHighway.value.plot <- ggplot(data2, aes(distanceToHighway, logHomeValue))
distanceHighway.value.plot = plot_scatterplot(distanceHighway.value.plot)

pupilTeacher.value.plot <- ggplot(data2, aes(pupilTeacherRatio, logHomeValue))
pupilTeacher.value.plot = plot_scatterplot(pupilTeacher.value.plot)

lowIncome.value.plot <- ggplot(data2, aes(pctLowIncome, logHomeValue))
lowIncome.value.plot = plot_scatterplot(lowIncome.value.plot)

pollution.value.plot <- ggplot(data2, aes(pollutionIndex, logHomeValue))
pollution.value.plot = plot_scatterplot(pollution.value.plot)

nBedrooms.value.plot <- ggplot(data2, aes(nBedRooms, logHomeValue))
nBedrooms.value.plot = plot_scatterplot(nBedrooms.value.plot)
```

#reconsider to use homeValue?

#Possible categorical factors in crimeRate_pc (x < 1), ageHouse (x > 95), distanceToCity, distanceToHighway


##Question 2

_Build a time-series model for the series in series02.txt and use it to perform a 24-step ahead forecast._

```{r}
series <- read.table("series02.txt")
series <- ts(series$V1)

#Visualize the data
par(mfrow = c(2,2))
plot.ts(series, col = "navy", xlab = "Time Period", ylab = "Values", main = "Time Series for Series 02")
hist(series, main = "Histogram of Values of Series 02")
acf(series, main = "ACF of Series 02")
pacf(series, main = "PACF of Series 02")
```

Notice the general structure of the series. There seems to be a long run average, where the values are fluctuating around a central axis but with with a major series of spikes in the beginning signaling serious volatility. There does not seem to be seasonality or a trend. The ACF interestingly shows a sharp drop after the 0 lag, but slightly statistically significant lags throughout the series. The PACF also shows slight significance at several lags after the most significant at what looks like the 3rd lag. 

We suspect there is non-constant variance present in this series, so we will plot a correlogram of the squared values of a mean adjusted version of this series (adjusted so the mean is zero). 

```{r}
par(mfrow = c(1,1))
acf((series - mean(series))^2, main = "ACF of Squared Terms")
```

The square values that are plotted are equivalent to the variance. What the statistically significant values indicate is that there is serial correlation, meaning conditional heteroskedasticity. In plain English, this means that the variance is not constant throughout the series, rather the variance depends on what window of time we are looking at. This violates a core assumption of stationarity, meaning we will have to use a non-stationary model to fit this data. 

```{r}
garch1 <- garch(series, trace = F)
resids <- garch1$residuals[-1] #first value is NA

par(mfrow = c(2,1))
acf(resids, main = "ACF of GARCH(1,1) Residuals")
acf(resids^2, main = "ACF of GARCH(1,1) Residuals^2")
Box.test(resids, type = "Ljung-Box")
garch1
t(confint(garch1))
```

Notice that with the ACF of both the series residuals and sqaured residuals there is no autocorrelation. This suggests the residuals are behaving like white noise and thus the model is a good fit. Examining the model itself 0 is not contained in the 95% confidence intervals for the coefficients, meaning that the coefficients are satatistically significant at the 95% level. 

#Question: Do we think that's good enough? Look into fitting GARCH better
#Forecasting

##Question 3

_Build a time-series model for the series in series03.csv and use it to perform a 24-step ahead forecast_

```{r}
#load data and do preliminary visualization
series <- read.csv("series03.csv")
series <- ts(series$X9.88)

par(mfrow = c(2,2))
plot.ts(series, xlab = "Time Period", ylab = "Value", main = "Time Series Plot of Series 03", col = "navy")
hist(series, main = "Histogram of Series 03")
acf(series, main = "ACF of Series 03")
pacf(series, main = "PACF of Series 03")
```

Notice from the time series plot that there is significant trend going on, long term upwards. The ACF shows significance through all past lags while the PACF is only siginficant for the first lag. There does not seem to be any seasonality. This looks like the realization of a random walk with drift process. 


```{r}
#plot different time series to suggest differencing
par(mfrow = c(3, 2))
plot.ts(series, main = "Original Time Series")
plot.ts(log(series), main = "Log of Time Series")
plot.ts(diff(series), main = "First Difference of Time Series")
plot.ts(diff(series, d = 2), main = "Second Difference of Time Series")
plot.ts(diff(log(series)), main = "First Difference of Log Time Series")
plot.ts(diff(log(series), d = 2), main = "Second Difference of Log Time Series")
```

It is clear from the original time series plot that the series is not stationary. Before proceeding to build a model we must render the series as stationary. 

```{r}
par(mfrow = c(2,2))
acf(diff(series), main = "ACF of First order Difference")
pacf(diff(series), main = "PACF of First order Difference")
acf(diff(series, d= 2), main = "ACF of Second order Difference")
pacf(diff(series, d = 2), main = "PACF of Second order Difference")

acf(diff(log(series)), main = "ACF of First Order Difference of Log")
pacf(diff(log(series)), main = "PACF of First Order Difference of Log")
acf(diff(log(series), d = 2), main = "ACF of Second Order Difference of Log")
pacf(diff(log(series), d = 2), main = "PACF of Second Order Difference of Log")
```

From examining these plots, it seems as though the second order difference provides the best transformation into white noise. In both cases the ACF shows a sharp cut off (suggesting an MA term) while the PACF gradually declines. The first order difference shows a lot of volatility in the PACF, suggesting correlations that are not easily captured. 
Between the second order difference and the second order difference of the log, the second order difference of the log seems to look more like white noise. There are fewer significant autocorrelations (which might be due to sampling) in the second order difference of the log and it decays more smoothly. Therefore, we will use the second order difference of the log to estimate the model. 

```{r}
get.best.arima <- function(x.ts, maxord = c(1,1,1))
{
  best.aic <- 1e8
  n <- length(x.ts)
  for (p in 0:maxord[1]) for (d in 0:maxord[2]) for (q in 0:maxord[3])
  {
    fit <- arima(x.ts, order = c(p, d, q), method = "ML")
    fit.aic <- -2 * fit$loglik + (log(n) + 1) * length(fit$coef)
    if (fit.aic < best.aic)
    {
      best.aic <- fit.aic
      best.fit <- fit
      best.model <- c(p, d, q)
    }
  }
  list(best.aic, best.fit, best.model)
}

auto.arima(log(series), allowdrift = FALSE)
mod <- auto.arima(log(series), d = 2)
mod
t(confint(mod))
```

Here we try using the auto.arima() function to find the best best. When using the auto.arima() function it suggests the first order difference of the log series. However, we saw above that this was not the best model examining the ACF and PACF so we instead specificed the order of differencing to be 2. When doing this, the suggested model is an ARIMA(2, 2, 1) model. However, examining the confidence intervals, we find that the 2 AR terms contain 0 in their confidence interval. That means we will fail to reject the null hypothesis these coefficients are 0. The MA term however does not contain 0 in its confidence interval and therefore we can reject the null hypothesis. Therefore, we will construct an ARIMA(0, 2, 1) model. 

```{r}
model <- arima(log(series), order = c(0, 2, 1))
model
t(confint(model))
```

0 is not contained in the confidence interval so this coefficient is statistically significant. 


```{r}
#diagnostic plots of residuals
resids <- model$residuals
plot.ts(resids, main = "Residuals of ARIMA Model")
hist(resids, main = "Histogram of Residuals")
acf(resids, main = "ACF of Residuals")
pacf(resids, main = "PACF of Residuals")
```

These residual diagnostics suggest a reasonably good aprpoximation of white noise. The ACF and PACF however do show quite a bit of volatility, so we will examine the squared residuals because we suspect there is non-constant variance. 

```{r}
par(mfrow = c(2,1))
acf(resids, main = "ACF of Residuals")
acf(resids^2, main = "ACF of Squared Residuals")
```

As we had suspected, the squared residuals show statistically significant terms at different intervals. Clearly, this suggests there is non-constant variance. Therefore, we will fit a GARCH model to the residuals. 

```{r}
resid.garch = garch(resids, trace = FALSE)
t(confint(resid.garch))
acf(resid.garch$residuals[-1], main = "ACF of GARCH fitted Residuals")
acf(resid.garch$residuals[-1]^2, main = "ACF of GARCH fitted Residuals^2")
```


The GARCH model shows statistically significant coefficients at the 95% level, because 0 is not contained in the confidence intervals. The ACFs of both the GARCH residuals and residuals squared show no significant values, meaning a good fit to the residuals. 

#Forecast? 
```{r}
get.best.arima.seas <- function(x.ts, maxord = c(1,1,1,1,1,1)) {
  best.aic <- 1e8
  n <- length(x.ts)
  for (p in 0:maxord[1]) for(d in 0:maxord[2]) for(q in 0:maxord[3])
    for (P in 0:maxord[4]) for(D in 0:maxord[5]) for(Q in maxord[6])
    {
      fit <- arima(x.ts, order = c(p, d, q), seas = list(order = c(P,D,Q), frequency(x.ts)), method = "CSS")
      fit.aic <- -2 * fit$loglik + (log(n) + 1) * length(fit$coef)
      if (fit.aic < best.aic)
      {
        best.aic <- fit.aic
        best.fit <- fit
        best.model <- c(p, d, q, P, D, Q)
      }
    }
  list(best.aic, best.fit, best.model)
}

```



##Question 4

_Build a time-series model for the series in series04.csv and use it to perform a 24-step ahead forecast. Possible models include AR, MA, ARMA, ARIMA, Seasonal ARIMA, GARCH, ARIMA-GARCH, or Seasonal ARIMA-GARCH models. Note that the original series may need to be transformed before it be modelled._

```{r}
#import data and run basic visualizations
series <- read.csv("series04.csv")
series <- ts(series$X25182)

par(mfrow = c(2,2))
plot.ts(series, xlab = "Time Period", ylab = "Values", main = "Time Series Plot of Series 04")
hist(series, main = "Histogram of Series 04")
acf(series, main = "ACF of Series 04")
pacf(series, main = "PACF of Series 04")
```

From the time series plot it should be obvious that there is seasonality in this series, suggesting seasonal lag terms will be needed. The series shows a general upwards trend, and we would argue this series is definitely not stationary. The ACF show statistically significant lags persisting but at different heights, further suggesting non-stationarity and seasonality. 

```{r}
par(mfrow = c(3,2))
plot.ts(series, xlab = "Time Period", ylab = "Values", main = "Time Series of Series 04 Original")
plot.ts(log(series), xlab = "Time Period", ylab = "Values", main = "Log of Series 04")
plot.ts(diff(series), xlab = "Time Period", ylab = "Values", main = "First Difference Series 04")
plot.ts(diff(log(series)), xlab = "Time Period", ylab = "Values", main = "First Difference Log Series 04")
plot.ts(diff(series, d = 2), xlab = "Time Period", ylab = "Values", main = "Second Difference Series 04")
plot.ts(diff(log(series), d =2), xlab = "Time Period", ylab = "Values", main = "Second Difference Log Series 04")
```

We are going to take a look at taking the log transformation and differencing of the series. What we want is ideally for the series to look like a white noise series (white noise by definition is stationary). At a glance, it appears the First Difference of the log of the series is a good candidate, it does not show varying levels of volatility through the series instead remaining largely uniform. Notice for example the first difference of the regular series shows changing volatility moving from left to right, characterized by the series fluctuating up and down more. This would suggest non-constant variance and thus the series would not be considered stationary. 

```{r}
#Examine ACF and PACF of First difference
par(mfrow = c(2,2))
acf(diff(series), main = "ACF of First Difference Series")
acf(diff(log(series)), main = "ACF of First Difference Log")
pacf(diff(series), main = "PACF of First Difference")
pacf(diff(log(series)), main = "PACF of First Difference Log")
```

Both the ACF and PACF of the first difference show highly significant lags at intervals of 6, suggesting a seasonal term is needed to capture this dynamic. This supports our earlier observation that there was seasonality present in this series. In particular the persistence of the lag in the ACF suggests a seasonal MA term at lag 6 would probably be appropriate. 


